\chapter{Modelli e Metodologie Utilizzate}
\label{cap:2}
Nel presente capitolo sono descritte le principali tecnologie e metodologie adottate durante lo sviluppo del progetto. La parte introduttiva fornisce una spiegazione concettuale dei concetti legati all'Intelligenza Artificiale, con focus specifico sui concetti di Object Detection e Bounding Box, essenziali per comprendere la natura del problema trattato. Si passa, poi, alla descrizione del linguaggio di programmazione e delle librerie di supporto, motivandone la scelta; particolare rilevanza è data a strumenti come Miniconda, Pytorch e CUDA, di fondamentale importanza nello sviluppo del lavoro di tesi. Il nucleo centrale del capitolo è dedicato all'architettura YOLOv7, con il suo funzionamento e le sue innovazioni rispetto ai modelli precedenti, caratteristiche che l'hanno reso la soluzione più adeguata alla sfida proposta. Infine, è descritta la libreria Norfair, utilizzata per il tracciamento, con il suo funzionamento e le sue caratteristiche.

\section{Panoramica sull'Intelligenza Artificiale, l'Object Detection e le Bounding Box}
Nel contesto del progetto di tesi, queste tecnologie sono state fondamentali: l'\textbf{Intelligenza Artificiale} ha permesso l'addestramento di un modello per un compito non usuale; l'\textit{Object Detection} è stata la metodologia per localizzare le regioni attive nei magnetogrammi; le \textit{Bounding Box} hanno rappresentato lo strumento pratico per delimitarle visivamente.

\subsection{Intelligenza Artificiale}
L'\textbf{Intelligenza Artificiale} (IA) è un campo dell'informatica che si propone di sviluppare sistemi in grado di svolgere attività tipicamente umane, quali ragionamento, apprendimento automatico, elaborazione del linguaggio naturale e percezione visiva\cite{russell2010artificial}.
Negli ultimi anni, l'IA ha compiuto enormi passi in avanti, e questo lo deve all'evoluzione del \textit{machine learning} (ML), e in particolare del \textit{deep learning} (DL). Tutto ciò ha reso possibile affrontare con successo problemi complessi come il riconoscimento facciale, la traduzione automatica, il rilevamento di oggetti e persino la guida autonoma \cite{goodfellow2016deep}. \\
Nel contesto dell’IA moderna, le tecniche più rilevanti sono:
\begin{itemize}
    \item \textbf{Machine Learning:} metodi che apprendono da dati osservati senza essere esplicitamente programmati per ogni compito.
    \item \textbf{Deep Learning:} architetture neurali multilivello, capaci di apprendere rappresentazioni gerarchiche e complesse dei dati.
    \item \textbf{Apprendimento supervisionato e non supervisionato:} rispettivamente con o senza etichette nei dati di input.
\end{itemize}

\subsubsection{Machine Learning}
Il \textbf{Machine Learning} (ML) è una branca dell’intelligenza artificiale che si occupa di progettare algoritmi in grado di apprendere automaticamente dai dati, migliorando le proprie prestazioni nel tempo senza essere esplicitamente programmati \cite{mitchell1997machine}.
Un algoritmo di Machine Learning è addestrato su un insieme di dati \textit{training set}, per poi essere valutato su dati non visti, detti \textit{test set}, così da poter verificarne la sua capacità di generalizzazione. \\
Le principali modalità di apprendimento nel ML sono:
\begin{itemize}
    \item \textbf{Apprendimento supervisionato:} l’algoritmo apprende da dati etichettati, ovvero ogni esempio presente dataset è associato ad un output già noto. Questa rappresenta la modalità più usata, soprattutto per quanto riguarda la classificazione di immagini o il riconoscimento vocale.
    \item \textbf{Apprendimento non supervisionato:} i dati non hanno etichette, per cui è l’algoritmo stesso a tentare di identificare strutture nascoste o raggruppamenti nei dati. Questa modalità è usata nel \textit{clustering}, nella compressione o nel rilevamento di anomalie.
    \item \textbf{Apprendimento per rinforzo:} un agente interagisce con un ambiente e apprende tramite un sistema di ricompense e penalità, ottimizzando le proprie decisioni. Questa modalità è impiegata, ad esempio, nei videogiochi o nella robotica autonoma.
\end{itemize}

\subsubsection{Deep Learning}
Il \textbf{Deep Learning} (DL), come accennato prima, è una sottoclasse del ML che utilizza \textit{reti neurali profonde}, composte da molti strati (\textit{layers}) nascosti. Tali modelli sono particolarmente efficienti nel trattare dati complessi e non strutturati, come immagini, audio e testo \cite{goodfellow2016deep}.
Grazie alla grande disponibilità di dati e potenza computazionale, il Deep Learning ha rivoluzionato il campo della \textit{computer vision}, rendendo possibili compiti prima irrisolvibili, tra cui:
\begin{itemize}
    \item Riconoscimento facciale in tempo reale
    \item Traduzione automatica basata sul contesto
    \item Rilevamento e classificazione di oggetti in immagini ad alta risoluzione
\end{itemize}

\subsection{Object Detection}
L'\textbf{Object Detection} è un'area fondamentale della \textit{computer vision} che ha l'obiettivo di localizzare e classificare automaticamente uno o più oggetti in un'immagine o in un video \cite{zhao2019object}.
I modelli di Object Detection producono come output sia la categoria di ciascun oggetto (es. ``persona'', ``auto'', ``segnale stradale''), sia una bounding box che ne racchiude l'area nell'immagine. \\
Le principali tecniche si dividono in due grandi famiglie:
\begin{itemize}
    \item \textbf{Two-stage detectors:} suddividono il processo in due fasi distinte: dapprima vi è la generazione di regioni proposte (\textit{Region Proposal Network, RPN}) e successivamente la classificazione delle regioni. Esempi noti sono R-CNN, Fast R-CNN e Faster R-CNN \cite{ren2015faster}.
    \item \textbf{One-stage detectors:} eseguono direttamente la classificazione e la regressione delle bounding box in un'unica fase, abbreviando notevolmente i tempi. I principali rappresentanti di questa famiglia sono YOLO (\textit{You Only Look Once}) - di cui è stato fatto uso per il presente progetto di tesi - \cite{redmon2016you} e SSD (\textit{Single Shot MultiBox Detector}) \cite{liu2016ssd}.
\end{itemize}

\subsubsection{Metriche di Valutazione}
Per valutare quantitativamente le prestazioni, è necessario stabilire un criterio oggettivo per classificare la correttezza delle predizioni. A tal fine si utilizza l'\textbf{Intersection over Union (IoU)}, che misura il grado di sovrapposizione geometrica tra la bounding box predetta dal modello e quella reale (ground truth), calcolata come il rapporto tra l'area dell'intersezione e l'area dell'unione delle due box:
  \[
  \text{IoU} = \frac{\text{Area of Overlap}}{\text{Area of Union}}
  \] \\
Fissata una soglia di tolleranza $\alpha$ (tipicamente $0.5$), ogni predizione viene classificata in una delle seguenti categorie:
\begin{itemize}
  \item \textbf{True Positive (TP)}: rilevamento corretto, caratterizzato da un $\text{IoU} \ge \alpha$.
  \item \textbf{False Positive (FP)}: rilevamento errato o "falso allarme", in cui il modello predice un oggetto dove non esiste o con una sovrapposizione insufficiente ($\text{IoU} < \alpha$).
  \item \textbf{False Negative (FN)}: mancato rilevamento, ovvero un oggetto reale presente nel dataset che il modello non è riuscito a identificare.
\end{itemize}
Sulla base di queste quantità, si definiscono le metriche derivate:
\begin{itemize}
  \item \textbf{Precision ($P = \frac{TP}{TP + FP}$)}: indica l'affidabilità del modello, esprimendo la percentuale di predizioni corrette rispetto al totale dei rilevamenti effettuati.
  \item \textbf{Recall ($R = \frac{TP}{TP + FN}$)}: misura la capacità di copertura del modello, indicando la frazione di oggetti reali correttamente individuati.
  \item \textbf{mAP (mean Average Precision)}: rappresenta la metrica globale di sintesi. Essa è calcolata come la media delle Average Precision (AP) di tutte le classi, dove l'AP corrisponde all'area sottesa alla curva Precision-Recall, offrendo una valutazione che bilancia sia la precisione che la recall al variare della soglia di confidenza.
\end{itemize}

\subsection{Bounding Box}
La \textbf{bounding box} è un rettangolo utilizzato per racchiudere un oggetto rilevato all'interno di un'immagine. È solitamente rappresentata da quattro coordinate:
\[
(x_{\text{min}}, y_{\text{min}}, x_{\text{max}}, y_{\text{max}})
\]
dove:
\begin{itemize}
    \item $(x_{\text{min}}, y_{\text{min}})$ rappresenta il vertice in alto a sinistra,
    \item $(x_{\text{max}}, y_{\text{max}})$ rappresenta il vertice in basso a destra.
\end{itemize}
Alternativamente, può essere espressa con centro e dimensioni:
\[
(x_{\text{center}}, y_{\text{center}}, w, h)
\]
dove $w$ e $h$ sono larghezza e altezza.\\
L’accuratezza delle bounding box è cruciale nei sistemi \textit{real-time}, dove la minima imprecisione può compromettere l’affidabilità dell'intero sistema.

\section{Python}
\textbf{Python} è un linguaggio di programmazione ad alto livello, ampiamente utilizzato per la sua semplicità di sintassi, versatilità e la presenza di un ecosistema ricco di librerie.
Spicca per i suoi punti di forza: adattabilità ad ogni piattaforma, interattività e dinamicità e protipizzazione rapida.\\
Nel contesto di questo progetto di tesi, Python è stato scelto come linguaggio principale per lo sviluppo degli script e l'esecuzione degli elaborazione dati, visione artificiale e deep learning. Si è rivelato essenziale per integrare le diverse componenti del sistema: dalla lettura dei dati scientifici HARP, alla pre-elaborazione delle immagini, fino all'addestramento ed inferenza del modello.

\subsection{Librerie ausiliarie}
\begin{itemize}
  \item \textbf{NumPy} \cite{numpy}: è la libreria fondamentale per il calcolo scientifico in Python. Fornisce strutture dati per la gestione di array multidimensionali e innumerevoli funzioni matematiche per operazioni vettoriali e matriciali. Nel progetto, è stata utilizzata per la manipolazione e preparazione dei dati, ovvero dei magnetogrammi, rappresentati come array multidimensionali, e per gestire le coordinate delle bounding box durante le fasi di addestramento e validazione.
  \item \textbf{SciPy} \cite{scipy}: estende le funzionalità di NumPy, offrendo strumenti per operazioni matematiche avanzate come integrazione numerica, ottimizzazione, interpolazione e algebra lineare. Nel progetto, è stata utilizzata per supportare calcoli complessi durante l’analisi dei dati e l’elaborazione dei risultati, in particolare ha fornito strumenti di supporto per analisi statistiche preliminari sulle proprietà magnetiche delle regioni attive descritte nei dati HARP.
  \item \textbf{OpenCV} \cite{opencv}: libreria open source per la computer vision e l’elaborazione delle immagini. Offre algoritmi ottimizzati per operazioni di filtraggio, rilevamento di caratteristiche, trasformazioni geometriche e manipolazione delle immagini. Nel progetto, è stata usata in primis perché viene utilizzata dal modello, ma anche per la pre-elaborazione e manipolazione dei magnetogrammi solari come, ad esempio, la normalizzazione dei valori e la conversione dei formati, rendendoli idonei per l'input da passare al modello.
  \item \textbf{Matplotlib} \cite{matplotlib}: libreria di visualizzazione dati che permette la creazione di grafici statici, animati e interattivi. Consente di rappresentare chiaramente e in modo personalizzabile i risultati ottenuti, facilitando l’analisi e la documentazione visiva dei dati elaborati. Nel progetto, è stata la libreria indispensabile per visualizzare i risultati, ad esempio per disegnare le bounding box predette direttamente sui magnetogrammi solari e per generare grafici che mostrano l'andamento delle performance del modello.
\end{itemize}

\section{Miniconda}
\textbf{Miniconda} è una distribuzione minimale di Conda, un sistema di gestione degli ambienti e dei pacchetti Python. È stato utilizzato per creare ambienti isolati e riproducibili, permettendo di configurare ciascuno di essi con una propria versione di Python e delle librerie di terze parti necessarie (come PyTorch, NumPy, OpenCV).\\
Questa gestione consente di mantenere sotto controllo le dipendenze e garantisce che le specifiche versioni delle librerie siano compatibili fra loro, evitando conflitti che avrebbero potuto compromettere l'addestramento del modello. \cite{miniconda}

\section{PyTorch}
\textbf{PyTorch} è un framework open-source per il deep learning sviluppato da Meta AI. Esso fornisce un'interfaccia dinamica e intuitiva per la definizione e l'ottimizzazione delle reti neurali, ed è particolarmente apprezzato per il suo supporto nativo all’accelerazione tramite GPU \cite{paszke2019pytorch}.\\
In questo progetto di tesi, è stato il framework su cui si basa l'intero processo di addestramento ed inferenza del modello, offrendo la flessibilità desiderata e necessaria per apportare le modifiche atte all'adattamento del compito scientifico.

\section{CUDA}
\textbf{CUDA} (\textit{Compute Unified Device Architecture}) è una piattaforma di calcolo parallelo sviluppata da NVIDIA, che consente l’utilizzo delle GPU per compiti generici di calcolo.\\
Data l'enorme mole di dati e la complessità del modello utilizzato, l'accelerazione hardware è stata indispensabile per ridurre i tempi sia di addestramento che di inferenza, rendendo possibile l'esecuzione di più esperimenti in tempi alquanto ragionevoli.\\
Nel contesto di questa tesi, la corretta configurazione di CUDA e la sua compatibilità con la versione di PyTorch si sono rivelate essenziali per il garantire il funzionamento ottimale dei modelli su GPU.\cite{cuda}.

\section{YOLOv7}
\textbf{YOLOv7} (\textit{You Only Look Once version 7}) è uno dei modelli più recenti e avanzati per il rilevamento oggetti in tempo reale, appartenente alla famiglia di algoritmi YOLO. Sviluppato per ottenere alte prestazioni sia in termini di accuratezza che di velocità, si distingue per una serie di ottimizzazioni che lo rendono adatto ad innumerevoli applicazioni, anche su dispositivi con risorse computazionali limitate \cite{wang2022yolov7}. \\
La scelta di tale modello è motivata dall'ottimo compromesso tra efficienza e precisione, una caratteristica cruciale nell'analisi di grandi moli di dati astrofisici.

\subsection{Funzionamento e Caratteristiche}
YOLOv7 è progettato per realizzare una \textit{pipeline end-to-end}, che riceve in input un’immagine e restituisce in output le bounding box e le classi associate agli oggetti rilevati. \\
Il funzionamento, durante la fase di inferenza, si basa sulla suddivisione dell'immagine in griglie e sull'applicazione di convoluzioni profonde per produrre:
\begin{itemize}
  \item Le coordinate delle bounding box.
  \item La classe predetta per ogni oggetto rilevato.
  \item La probabilità (\textit{confidenza}) associata a ciascuna predizione.
\end{itemize}
Si tratta del cuore del sistema di rilevamento sviluppato nel presente lavoro di tesi, necessario per il successivo tracciamento.\\
L'architettura separa nettamente la configurazione degli iperparametri e degli \textit{anchor} dai pesi pre-addestrati e offre diverse caratteristiche avanzate:
\begin{itemize}
  \item \textbf{Efficienza}: elevata precisione nel rilevamento pur mantenendo alte velocità di inferenza.
  \item \textbf{Versatilità}: supporto per \textit{Object Detection}, \textit{Instance Segmentation} e \textit{Pose Estimation}.
  \item \textbf{Scalabilità}: disponibilità di architetture derivate (ad esempio YOLOv7-tiny, YOLOv7-X, YOLOv7-W6) adattabili a diverse situazioni e capacità di calcolo.
  \item \textbf{Ottimizzazione Hardware}: supporto nativo per GPU CUDA, che consente un'inferenza efficiente su hardware NVIDIA.
  \item \textbf{Multitasking}: possibilità di usare \textit{multi-head} per compiti multitask (ad esempio detection + keypoints).
\end{itemize}
In fase di addestramento, inoltre, il modello utilizza tecniche avanzate di ottimizzazione che migliorano l’apprendimento, senza aumentare la complessità dell’inferenza.

\subsection{Avanzamenti nei Detector in Tempo Reale}
YOLOv7 rappresenta un significativo passo avanti nel campo del \textit{real-time object detection}. L’architettura introduce il concetto di \textbf{Trainable Bag-of-Freebies}: un insieme di moduli e strategie ottimizzate che migliorano significativamente la fase di addestramento senza incrementare il costo computazionale durante l'inferenza. \\
Tutti i modelli YOLO sono stati addestrati da zero sul dataset COCO, senza utilizzare pesi pre-addestrati o dati esterni \cite{wang2022yolov7}.

\subsubsection{E-ELAN: Extended Efficient Layer Aggregation Networks}
Per migliorare l’apprendimento delle rappresentazioni senza interrompere il percorsi di propagazione del gradiente, YOLOv7 introduce una nuova \textit{backbone} denominata \textbf{E-ELAN}, visibile in Figura \ref{fig:architettura_eelan} \\
A differenza delle architetture tradizionali, E-ELAN mantiene fissa la struttura di transizione, ma agisce sui blocchi computazionali utilizzando:
\begin{itemize}
  \item \textbf{Group Convolution}: per suddividere i canali in gruppi.
  \item \textbf{Shuffle \& Merge Cardinality}: per mescolare e fondere le \textit{feature map} provenienti dai diversi gruppi.
\end{itemize}
\begin{figure}[tb]
  \centering
  \includegraphics[width=1.0\textwidth]{Figs/Cap2/e-elan.png}
  \caption[Architettura E-ELAN]{L’architettura E-ELAN mantiene invariato il percorso di propagazione del gradiente dell’architettura originale, ma utilizza convoluzioni di gruppo per aumentare la cardinalità delle feature aggiunte. Le feature provenienti da diversi gruppi vengono combinate attraverso operazioni di mescolamento e fusione, migliorando così la varietà delle rappresentazioni apprese e l’efficienza nell’uso dei parametri e del calcolo.}
  \label{fig:architettura_eelan}
\end{figure}

\subsubsection{Compound Scaling}
Il ridimensionamento dei modelli avviene, solitamente, modificando profondità, larghezza o risoluzione.\\
Tuttavia, nei modelli basati su concatenazione (come YOLO), scalare solo la profondità causa una variazione indesiderata dei canali di input nei layer successivi, rompendo l'equilibrio computazionale (Figura \ref{fig:scaling}).\\
Per risolvere questo problema, YOLOv7 propone il \textbf{Compound Scaling}, una tecnica che scala profondità e larghezza in modo congiunto: quando si scala la profondità di un blocco computazionale, si scala simultaneamente la larghezza dei layer di transizione.\\
Le relazioni sono definite come:
\[
\begin{aligned}
d' &= d \cdot \alpha \\
w' &= w \cdot \beta
\end{aligned}
\]
dove \(\alpha\) e \(\beta\) sono fattori empiricamente scelti per mantenere l'architettura ottimale al variare della scala del modello.
\begin{figure}[tb]
  \centering
  \includegraphics[width=1.0\textwidth]{Figs/Cap2/compound-scaling.png}
  \caption[Compound Scaling]{Comportamento e la soluzione proposta per lo scaling nei modelli basati su concatenazione, composta da tre sottoparti: (a): Dimostra che scalare solo la profondità (depth) in un blocco concatenativo fa aumentare la larghezza del layer di uscita; (b): Mostra che l’output più largo influenza la transizione successiva, creando problemi strutturali; (c): Presenta la soluzione proposta: uno scaling combinato (compound scaling), in cui si scala la profondità nel blocco e la larghezza nei layer di transizione per mantenere la coerenza architetturale.}
  \label{fig:scaling}
\end{figure}

\subsubsection{Planned Re-parameterized Convolution}
YOLOv7 ottimizza le convoluzioni ri-parametrizzate (\textit{RepConv}), che combinano diverse operazioni in un unico layer per l'inferenza.\\
Gli autori hanno osservato che la connessione identità -tipica delle RepConv standard- degrada le prestazioni, se applicata indiscriminatamente su connessioni residue o concatenate. La soluzione adottata è, pertanto, la \textbf{Planned RepConv}, che utilizza una variante priva di identità (\textit{RepConvN}) nei layer sensibili, garantendo la compatibilità strutturale con architetture come ResNet o DenseNet.

\subsubsection{Label Assignment: Coarse-to-Fine Lead Guided}
Una delle innovazioni più rilevanti riguarda la strategia di assegnazione delle etichette (\textbf{label assignment}), durante l'addestramento, con supervisione profonda (\textit{deep supervision}), visibile in Figura \ref{fig:assegnazione_etichette}.\\
Il modello utilizza due "teste" (\textit{heads}):
\begin{itemize}
  \item \textbf{Lead Head}: responsabile dell'output finale, genera predizioni raffinate.
  \item \textbf{Auxiliary Head}: assiste l'addestramento nei livelli intermedi, ricevendo etichette guidate dal lead head, non direttamente dal ground truth.
\end{itemize}
La strategia \textbf{Coarse-to-Fine} prevede che il Lead Head guidi l'apprendimento dell'Auxiliary Head:
\begin{enumerate}
  \item Il Lead Head genera etichette di alta precisione per se stesso.
  \item Dalle predizioni del Lead Head vengono derivate etichette grezze per l'Auxiliary Head. Per evitare che quest'ultima faccia \textit{overfitting}, viene applicato un vincolo superiore (\textit{Upper Bound Constraint}) che limita l'assegnazione delle etichette in base alla distanza dal centro dell'oggetto:
    \[
    \text{Score}_{\text{coarse}} = \max \left( 0, 1 - \frac{\text{dist}}{\text{thresh}} \right)
    \]
\end{enumerate}
Questo permette alla testa ausiliaria di apprendere meglio le informazioni contestuali, migliorando la capacità di generalizzazione del modello (\textit{recall}), mentre la testa principale si focalizza sulla precisione.
\begin{figure}[tb]
  \centering
  \includegraphics[width=1.0\textwidth]{Figs/Cap2/coarse-fine-labels.png}
  \caption[Strategia di Label Assignment]{Assegnazione di etichette coarse per l’head ausiliario e fine per il lead head. Rispetto al modello standard (a), lo schema (b) include una testa ausiliaria. Diversamente dall’assegnazione indipendente delle etichette (c), sono proposte due nuove strategie: (d) assegnazione guidata dal lead head e (e) assegnazione guidata coarse-to-fine. Quest’ultima genera contemporaneamente le etichette per il training del lead head e della testa ausiliaria combinando le predizioni del lead head con il ground truth.}
  \label{fig:assegnazione_etichette}
\end{figure}

\subsubsection{Altri Trainable-of-Freebies}
Oltre alle innovazioni architetturali, YOLOv7 integra una serie di "trucchi" di addestramento (\textit{Bag-of-Freebies}) ottimizzati per massimizzare le prestazioni senza costi aggiuntivi:
\begin{itemize}
    \item \textbf{Batch Normalization fusa}: durante l'inferenza, i layer di Batch Normalization vengono fusi con i layer convoluzionali, semplificando la struttura della rete e riducendo la latenza.
    \item \textbf{Implicit Knowledge}: un concetto derivato da YOLOR, qui semplificato tramite l'uso di vettori statici pre-calcolati che vengono combinati con le feature map.
    \item \textbf{EMA Model (Exponential Moving Average)}: utilizzo di una media mobile esponenziale dei pesi del modello durante il training per ottenere il modello finale di test, tecnica che aumenta la robustezza e la stabilità delle predizioni.
\end{itemize}

\subsubsection{Confronto delle Prestazioni}
Le innovazioni introdotte permettono a YOLOv7 di superare i modelli precedenti. Come evidenziato nella Tabella \ref{tab:yolo_comparison}, a parità di risoluzione, YOLOv7 ottiene risultati migliori in mAP, mantenendo un \textit{framerate} (FPS) più elevato rispetto a YOLOR e YOLOv5.
\begin{table}[tb]
\centering
\begin{tabular}{lcccc}
\hline
Modello & FPS & mAP & Parametri & FLOPs \\
\hline
YOLOv5-X & 83 & 50.7\% & 86.7M & 205.7G \\
YOLOR-CSP-X & 87 & 52.7\% & 96.9M & 226.8G \\
\textbf{YOLOv7-X} & \textbf{114} & \textbf{52.9\%} & 71.3M & 189.9G \\
\hline
\end{tabular}
\caption[Confronto prestazioni YOLOv7]{Confronto tra YOLOv7 ed altri modelli stato dell'arte (640×640).}
\label{tab:yolo_comparison}
\end{table}
La Tabella \ref{tab:scaling_comparison} dimostra, inoltre, l'efficacia del Compound Scaling: rispetto allo scaling tradizionale (solo larghezza o solo profondità), il metodo combinato ottiene il miglior risultato in accuratezza con un incremento minimo di calcoli.
\begin{table}[tb]
\centering
\begin{tabular}{lccc}
\hline
Modello & mAP & Parametri & FLOPs \\
\hline
Base & 51.7\% & 47.0M & 125.5G \\
Solo larghezza & 52.4\% & 73.4M & 195.5G \\
Solo profondità & 52.7\% & 69.3M & 187.6G \\
\textbf{Compound (YOLOv7-X)} & \textbf{52.9\%} & 71.3M & 189.9G \\
\hline
\end{tabular}
\caption[Efficienza del Compound Scaling]{Confronto dell'efficienza del Compound Scaling e metodi di scaling tradizionali.}
\label{tab:scaling_comparison}
\end{table}

\section{Norfair}
\textbf{Norfair} è una libreria open source sviluppata in Python per il \textit{tracking} multi-oggetto in tempo reale \cite{norfair}. Essa è progettata per integrarsi modularmente con qualsiasi sistema di rilevamento che fornisca coordinate spaziali (ad esempio le coordinate \((x, y)\) dei centri delle bounding box). \\
La libreria si occupa esclusivamente della parte di tracking, ovvero dell'associazione temporale dei rilevamenti (\textit{detections}) frame per frame, mantenendo un identificatore univoco stabile per ogni oggetto monitorato nel video o nel flusso di immagini (esempio in Figura \ref{fig:esempio_tracking}). Non includendo la componente di rilevamento, essa offre la flessibilità di utilizzare qualsiasi \textit{detector} esterno (come YOLO, Detectron2, MediaPipe).\\
Nel progetto sviluppato, svolge il ruolo chiave di "inseguire" le regioni attive nel tempo: una volta che YOLOv7 le ha identificate nel singolo magnetogramma, Norfair associa le rilevazioni tra frame consecutivi, dando la possibilità di studiare l'evoluzione di una specifica regione solare. 

\subsection{Funzionamento e Caratteristiche}
Il funzionamento di Norfair si basa sui seguenti concetti chiave:
\begin{itemize}
  \item \textbf{Detections}: ogni oggetto rilevato in un fotogramma è rappresentato da coordinate spaziali, tipicamente il centro della bounding box o altri punti di interesse.
  \item \textbf{Tracks}: sono le tracce temporali che mantengono lo storico delle posizioni e degli identificatori degli oggetti nel tempo.
  \item \textbf{Funzione di distanza}: per associare i nuovi rilevamenti alle tracce esistenti, viene utilizzata una funzione di distanza personalizzabile (ad esempio euclidea o basata su caratteristiche visive).
  \item \textbf{Assegnazione}: ogni nuova detection viene assegnata alla traccia più vicina purché la distanza sia inferiore a una soglia predefinita (\textit{distance threshold}); in caso contrario, viene creata una nuova traccia.
\end{itemize}
Ad ogni nuovo fotogramma, Norfair riceve l’elenco delle \textit{detections}, aggiorna le tracce esistenti o ne crea di nuove e rimuove quelle non aggiornate per un certo numero di frame, garantendo così un tracking coerente anche in presenza di occlusioni temporanee o movimenti rapidi.
\begin{figure}[tb]
  \centering
  \includegraphics[width=1\textwidth]{Figs/Cap2/traffic.jpg}
  \caption{Esempio di multi-tracking di Norfair}
  \label{fig:esempio_tracking}
\end{figure}



