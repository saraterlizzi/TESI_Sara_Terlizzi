\chapter{Codice Sorgente}
\label{app:codice}

In questa appendice sono riportati i codici sorgente completi sviluppati per il progetto.

\section{Generazione del Dataset e Splitting Temporale}
\label{sec:dataset_split_code}
\begin{lstlisting}[language=Python, caption={Script dataset.py per il campionamento e la generazione degli split}, label={lst:dataset_generation}]
import pandas as pd  # Libreria per la manipolazione di dati tabellari (DataFrame).
import numpy as np  # Libreria per calcoli numerici e gestione array.
import matplotlib.pyplot as plt  # Libreria per la creazione di grafici.
from datetime import datetime, timedelta  # Classi per la gestione di date e intervalli di tempo.

# --- CARICAMENTO DATI ---
# URL del dataset ufficiale SILSO (Sunspot Index and Long-term Solar Observations).
url = 'https://www.sidc.be/SILSO/DATA/SN_m_tot_V2.0.txt'
# Definizione dei nomi delle colonne per il file di testo (senza header).
columns = ['Year', 'Month', 'FracDate', 'Sunspots', 'StdDev', 'Obs', 'DefProv']
# Legge il file CSV (formato testo separato da spazi) nell'oggetto DataFrame.
df = pd.read_csv(url, delim_whitespace=True, names=columns)
# Crea una colonna 'Date' convertendo Anno e Mese in oggetti datetime (giorno impostato a 1).
df['Date'] = pd.to_datetime(df[['Year', 'Month']].assign(day=1))

# --- FILTRAGGIO TEMPORALE ---
# Filtra i dati per includere solo il Ciclo Solare 24 (Dic 2008 - Dic 2019).
cycle24 = df[(df['Date'] >= '2008-12-01') & (df['Date'] <= '2019-12-31')]

# --- CONFIGURAZIONE CAMPIONAMENTO ---
# Definisce le proporzioni desiderate per ogni sottoinsieme del dataset.
proportions = {'Train': 0.7, 'Validation': 0.15, 'Test': 0.15}

# Regola la densita' di campionamento per anno in base all'attivita' solare.
# Assegna peso doppio agli anni di massimo solare (2012-2014) per catturare piu' regioni attive.
sampling_weights = {year: 2 if 2012 <= year <= 2014 else 1 for year in range(2011, 2020)}
# Crea una lista di anni ripetuti in base al loro peso (es. 2012 appare 2 volte).
repeated_years = [year for year in range(2011, 2020) for _ in range(sampling_weights[year])]
np.random.seed(42)  # Imposta il seme random per la riproducibilita'.
np.random.shuffle(repeated_years)  # Mescola l'ordine degli anni da campionare.

# --- LOGICA DI SELEZIONE PERIODI ---
period_length = 90  # Durata in giorni di ogni blocco (Train + Val + Test).
separation_days = 15  # Giorni di "cuscinetto" tra i set per evitare data leakage (correlazione temporale).
selected_periods = {'Train': [], 'Validation': [], 'Test': []}  # Dizionario per salvare i risultati.
used_ranges = []  # Lista per tracciare i periodi gia' occupati ed evitare sovrapposizioni.

# Itera su ogni anno selezionato dalla lista pesata.
for year in repeated_years:
    year_start = datetime(year, 1, 1)  # Inizio dell'anno corrente.
    year_end = datetime(year, 12, 31)  # Fine dell'anno corrente.

    # Calcola l'ultimo giorno utile per iniziare un periodo che stia dentro l'anno.
    max_start_day = (year_end - year_start).days - period_length
    if max_start_day <= 0:
        continue  # Salta l'anno se non c'e' abbastanza spazio.

    attempts = 0  # Contatore tentativi per trovare uno spazio libero.
    while attempts < 100:
        # Sceglie un giorno di partenza casuale nell'anno.
        start_offset = np.random.randint(0, max_start_day)
        base_start = year_start + timedelta(days=start_offset)
        
        # Calcola le date di inizio e fine per il Training set.
        train_end = base_start + timedelta(days=int(proportions['Train'] * period_length))
        
        # Calcola le date per la Validazione (dopo i giorni di separazione).
        val_start = train_end + timedelta(days=separation_days)
        val_end = val_start + timedelta(days=int(proportions['Validation'] * period_length))
        
        # Calcola le date per il Test (dopo ulteriore separazione).
        test_start = val_end + timedelta(days=separation_days)
        test_end = test_start + timedelta(days=int(proportions['Test'] * period_length))

        # --- CONTROLLO SOVRAPPOSIZIONI ---
        # Verifica se l'intervallo proposto [base_start, test_end] tocca intervalli gia' usati.
        overlap = any(
            not (test_end < rng[0] or base_start > rng[1])
            for rng in used_ranges
        )

        # Se non c'e' sovrapposizione e finiamo entro l'anno corrente:
        if not overlap and test_end <= year_end:
            # Aggiunge le tuple (start, end) alle liste appropriate.
            # Nota: sottrae 1 giorno alla fine per avere estremi inclusivi precisi.
            selected_periods['Train'].append((base_start, train_end - timedelta(days=1)))
            selected_periods['Validation'].append((val_start, val_end - timedelta(days=1)))
            selected_periods['Test'].append((test_start, test_end - timedelta(days=1)))
            
            # Registra l'intero blocco come "usato".
            used_ranges.append((base_start, test_end))
            break  # Esce dal ciclo while (successo).

        attempts += 1  # Riprova se c'era sovrapposizione.

# --- ESPORTAZIONE DATI ---
splits_data = []
# Riorganizza i dati in un formato tabellare.
for split, periods in selected_periods.items():
    for start, end in periods:
        splits_data.append({'Split': split, 'Start': start, 'End': end})

# Crea un DataFrame e salva il piano di campionamento su CSV.
splits_df = pd.DataFrame(splits_data)
splits_df.to_csv('solar_cycle_splits.csv', index=False)

# --- VISUALIZZAZIONE ---
fig, ax = plt.subplots(figsize=(15, 7))
# Traccia l'andamento delle macchie solari nel tempo.
ax.plot(cycle24['Date'], cycle24['Sunspots'], label='Monthly Sunspot Number', color='blue')

# Aggiunge aree colorate per evidenziare i periodi selezionati.
colors = {'Train': 'green', 'Validation': 'yellow', 'Test': 'red'}
for split, periods in selected_periods.items():
    for start, end in periods:
        # axvspan disegna una banda verticale colorata.
        ax.axvspan(start, end, color=colors[split], alpha=0.3, label=f'{split} Period' if start == periods[0][0] else "")

# --- STIMA DEL VOLUME DATI ---
records_count = {}
for split, periods in selected_periods.items():
    # Calcola il numero stimato di immagini: giorni * 24h * 5 (1 immagine ogni 12 min).
    count = sum([(end - start).days * 24 * 5 for start, end in periods])
    records_count[split] = count
    print(f'{split} dataset contains {count} images.')

# Mostra i conteggi direttamente sul grafico.
for idx, (split, count) in enumerate(records_count.items()):
    ax.text(0.02, 0.9 - idx * 0.05, f'{split} images: {count}', transform=ax.transAxes, fontsize=12, color=colors[split])

# Configurazioni finali del grafico (titoli, label, griglia).
ax.set_title('Solar Cycle 24 Sunspot Numbers with Randomized Train-Validation-Test Sampling (2011-2019)')
ax.set_xlabel('Year')
ax.set_ylabel('Sunspot Number')
ax.grid(True)
ax.legend()
plt.tight_layout()
# Salva il grafico risultante su file PNG.
plt.savefig('splits.png', dpi=300)

# Stampa finale di riepilogo in console.
for idx, (split, count) in enumerate(records_count.items()):
    print(f'{split} images: {count}')
\end{lstlisting}

\section{Data Loader Custom}
\begin{lstlisting}[language=Python, caption={Classe DatasetH5 per il caricamento dei file .h5}, label={lst:dataset_h5_full}]
# --- BLOCCO IMPORTAZIONI ---
import torch  # Libreria principale per il deep learning.
from torch.utils.data import Dataset  # Classe base di PyTorch per creare dataset personalizzati.
import h5py  # Libreria specifica per leggere file in formato HDF5.
import numpy as np  # Libreria per il calcolo numerico, usata per manipolare gli array di dati.
import cv2  # Libreria OpenCV per operazioni sulle immagini, come il ridimensionamento.
import os  # Libreria per interagire con il sistema operativo.
import glob  # Libreria per trovare file che corrispondono a un pattern.
from tqdm import tqdm  # Libreria per gestire visivamente barre di avanzamento.

# --- DEFINIZIONE DELLA CLASSE DATASET ---
# Eredita da 'Dataset' di PyTorch per integrarsi con i suoi strumenti, come il DataLoader.
class DatasetH5(Dataset):
    # --- METODO COSTRUTTORE (`__init__`) ---
    # Viene eseguito una sola volta all'inizio. Prepara il dataset.
    def __init__(self, path, img_size=640, clip_range=(-1500, 1500)):
        # Salva i parametri di configurazione come attributi della classe.
        self.img_size = img_size  # Dimensione finale delle immagini.
        self.clip_min, self.clip_max = clip_range  # Intervallo per il clipping dei valori dei pixel.
        self.class_id = 0  # ID di classe fisso (0), dato che abbiamo solo una classe ("regione attiva").
        
        # --- LOGICA DI CACHING ---
        # Definisce una sottocartella 'cache' dove verranno salvati i dati pre-elaborati.
        cache_dir = 'cache'
        # Crea la cartella 'cache' se non esiste gia'. 'exist_ok=True' evita errori se la cartella esiste.
        os.makedirs(cache_dir, exist_ok=True)
        
        # Costruisce un nome univoco per i file di cache basato sul nome della cartella dei dati (es. "Train" o "Validation").
        cache_name = os.path.basename(os.path.normpath(path))
        # Crea il percorso completo per il file di cache delle etichette (es. 'cache/Train_labels.npy').
        label_cache = os.path.join(cache_dir, f'{cache_name}_labels.npy')
        # Crea il percorso completo per il file di cache delle dimensioni originali delle immagini.
        shape_cache = os.path.join(cache_dir, f'{cache_name}_shapes.npy')

        # Cerca tutti i file .h5 nel percorso dato, li ordina e ne salva la lista.
        self.h5_files = sorted(glob.glob(os.path.join(path, '*.h5')))
        # Salva il numero totale di file trovati.
        self.n = len(self.h5_files)

        # Controlla se entrambi i file di cache esistono gia'.
        if os.path.exists(label_cache) and os.path.exists(shape_cache):
            # --- CARICAMENTO VELOCE DA CACHE (AVVII SUCCESSIVI) ---
            print(f"Caricamento rapido da cache per '{cache_name}'...")
            # Carica l'array delle etichette dal file .npy. 
            # 'allow_pickle=True' e' necessario perche' le etichette sono in una lista di array.
            self.labels = np.load(label_cache, allow_pickle=True)
            # Carica l'array delle dimensioni dal file .npy.
            self.shapes = np.load(shape_cache)
            print(f"Cache caricata per {len(self.labels)} file. Avvio del training...")
        else:
            # --- CREAZIONE DELLA CACHE (PRIMO AVVIO LENTO) ---
            print(f"Cache non trovata. Creazione della cache per '{cache_name}' (lento solo la prima volta)...")
            
            # Inizializza le liste che conterranno i dati estratti.
            self.labels = []
            self.shapes = []
            bad_labels_count = 0  # Contatore per le etichette scartate.
            
            # Itera su ogni file .h5 trovato, mostrando una barra di avanzamento.
            for h5_path in tqdm(self.h5_files, desc=f"Caching metadata from {path}"):
                try:  # Blocco per gestire errori di lettura dei singoli file.
                    # Apre il file .h5 in modalita' lettura. 'with' assicura la chiusura automatica.
                    with h5py.File(h5_path, 'r') as f:
                        # Estrae il dataset del magnetogramma.
                        magnetogram_data = f['magnetogram/data']
                        # Legge le dimensioni originali (altezza, larghezza).
                        orig_h, orig_w = magnetogram_data.shape
                        # Aggiunge le dimensioni alla lista 'self.shapes'.
                        self.shapes.append([orig_h, orig_w])

                        # Accede al gruppo dei metadati HARP.
                        harp_group = f['harp/metadata']
                        image_labels = []  # Lista temporanea per le etichette di questa immagine.
                        
                        # Itera su ogni regione attiva trovata nei metadati.
                        for harp_id in harp_group:
                            # Estrae gli attributi della regione attiva corrente.
                            harp_attrs = harp_group[harp_id].attrs
                            
                            # Definisce le chiavi necessarie per calcolare una bounding box.
                            required_keys = ['CRPIX1', 'CRPIX2', 'CRSIZE1', 'CRSIZE2']
                            # Controlla se tutti gli attributi necessari sono presenti.
                            if not all(key in harp_attrs for key in required_keys):
                                continue  # Se ne manca uno, salta questa regione.

                            # Converte le dimensioni in numeri decimali.
                            w_abs = float(harp_attrs['CRSIZE1'])
                            h_abs = float(harp_attrs['CRSIZE2'])

                            # Controlla che le dimensioni siano positive.
                            if w_abs <= 0 or h_abs <= 0:
                                bad_labels_count += 1
                                continue  # Se non lo sono, scarta l'etichetta.

                            # Calcola le coordinate del centro e le dimensioni, normalizzandole rispetto alle dimensioni dell'immagine 
                            x_center_norm = float(harp_attrs['CRPIX1'] + w_abs / 2) / orig_w
                            y_center_norm = float(harp_attrs['CRPIX2'] + h_abs /2) / orig_h
                            width_norm = w_abs / orig_w
                            height_norm = h_abs / orig_h

                            # Controlla che il centro della bounding box sia dentro l'immagine.
                            if not (0.0 < x_center_norm < 1.0 and 0.0 < y_center_norm < 1.0):
                                bad_labels_count += 1
                                continue  # Se e' fuori, scarta l'etichetta.
                            
                            # Aggiunge l'etichetta valida (formato YOLO) alla lista temporanea.
                            image_labels.append([self.class_id, x_center_norm, y_center_norm, width_norm, height_norm])
                        
                        # Aggiunge le etichette di questa immagine alla lista principale.
                        self.labels.append(np.array(image_labels, dtype=np.float32) if image_labels else np.empty((0, 5), dtype=np.float32))

                except Exception as e:  # Se si verifica un errore grave durante la lettura.
                    print(f"Errore grave durante la lettura del file {h5_path}: {e}")
                    # Aggiunge placeholder per mantenere l'allineamento degli indici.
                    self.labels.append(np.empty((0, 5), dtype=np.float32))
                    self.shapes.append([0, 0])

            # Stampa un riepilogo delle etichette scartate, se ce ne sono.
            if bad_labels_count > 0:
                print(f"ATTENZIONE: Trovate e scartate {bad_labels_count} etichette corrotte.")

            # Converte la lista di liste 'self.shapes' in un unico array NumPy.
            self.shapes = np.array(self.shapes, dtype=np.float64)
            
            # SALVA I DATI PROCESSATI NELLA CACHE PER USO FUTURO.
            print(f"Salvataggio della cache in '{path}'...") # NOTA: Stampa il percorso dei dati, non della cache
            np.save(label_cache, self.labels)  # Salva le etichette.
            np.save(shape_cache, self.shapes)  # Salva le dimensioni.
            print("Cache creata. I prossimi avvii saranno istantanei.")

    # --- METODO `__len__` ---
    # Restituisce il numero totale di campioni nel dataset.
    def __len__(self):
        return self.n  # Restituisce il numero di file contati all'inizio.

    # --- METODO `__getitem__` ---
    # Carica e restituisce un singolo campione (immagine + etichetta) dato un indice.
    def __getitem__(self, index):
        # Ottiene percorso e etichette pre-caricate per l'indice richiesto.
        h5_path = self.h5_files[index]
        labels_tensor = torch.from_numpy(self.labels[index])
        
        try:  # Blocco per gestire errori di apertura file (es. file corrotti).
            # Tenta di aprire il file H5 e leggere i dati dell'immagine.
            with h5py.File(h5_path, 'r') as f:
                data = f['magnetogram/data'][:]  # Carica l'intero array in memoria.

            # Pulisce i dati da eventuali valori non numerici (NaN/inf).
            if np.isnan(data).any() or np.isinf(data).any():
                data = np.nan_to_num(data, nan=0.0, posinf=0.0, neginf=0.0)
            
            # Pre-processa l'immagine: clipping, normalizzazione e ridimensionamento.
            clipped_data = np.clip(data, self.clip_min, self.clip_max)
            normalized_data = (clipped_data - self.clip_min) / (self.clip_max - self.clip_min)
            resized_image = cv2.resize(normalized_data, (self.img_size, self.img_size), interpolation=cv2.INTER_LINEAR)
            
            # Converte a 3 canali (duplicando il canale unico) per compatibilita' con YOLOv7.
            image_rgb = np.stack([resized_image] * 3, axis=-1)
            # Converte l'array NumPy in un tensore PyTorch e riordina le dimensioni in [C, H, W].
            image_tensor = torch.from_numpy(image_rgb.transpose(2, 0, 1)).float()
            
            # Restituisce il campione completo.
            return image_tensor, labels_tensor, h5_path, self.shapes[index]

        except Exception as e:  # Se si verifica un qualsiasi errore durante il caricamento.
            # Stampa un avviso e ignora il dato corrotto.
            print(f"\nATTENZIONE: Ignorato file corrotto o illeggibile: {os.path.basename(h5_path)}")
            
            # Restituisce un'immagine nera per non interrompere il training.
            placeholder_image = torch.zeros((3, self.img_size, self.img_size))
            return placeholder_image, labels_tensor, h5_path, self.shapes[index]
\end{lstlisting}

\section{Scaricamento e Processamento Dati Solari}
\label{sec:download_code}
\begin{lstlisting}[language=Python, caption={Script download.py per il recupero dei dati da JSOC}, label={lst:download_script}]
import pandas as pd  # Libreria per leggere il file CSV con gli split temporali.
import os  # Libreria per operazioni sul file system (creazione cartelle, eliminazione file).
import drms  # Client ufficiale per accedere ai dati solari JSOC/DRMS.
import h5py  # Libreria per creare e gestire file HDF5 compressi.
import time  # Libreria per gestire pause e timeout.
import sunpy.map  # Libreria specifica per leggere e manipolare mappe solari (file FITS).
from sunpy.net import jsoc  # Modulo di SunPy per interfacciarsi con JSOC.
import multiprocessing as mp  # Libreria per il calcolo parallelo (gestione processi).
from datetime import datetime, timedelta  # Classi per la gestione temporale.

# Imposta l'email necessaria per effettuare richieste ai server DRMS.
EMAIL = 'edoria2011@gmail.com'

# Classe personalizzata che estende h5py.File per una scrittura sicura.
# Garantisce che non rimangano file corrotti su disco in caso di crash.
class SafeH5File(h5py.File):
    def __init__(self, filename, *args, **kwargs):
        super().__init__(filename, *args, **kwargs)
        self._store_filename = filename  # Memorizza il percorso del file.

    # Metodo chiamato automaticamente all'uscita dal blocco 'with'.
    def __exit__(self, exc_type, exc_val, exc_tb):
        self.close()  # Chiude il file.
        # Se si e' verificata un'eccezione (errore) durante la scrittura...
        if exc_type:
            print("An error occurred, cleaning up...")
            # ...cancella il file parziale/corrotto dal disco.
            if os.path.exists(self._store_filename):
                os.remove(self._store_filename)
        return False

# Funzione per gestire i download falliti con tentativi ripetuti (retry logic).
def export_with_retries(client, query, sleep_time=3, max_retries=100):
    retries = 0
    while retries < max_retries:
        try:
            # Tenta di esportare i dati richiedendo un URL FITS.
            response = client.export(query, method='url', protocol='fits')
            # Status 4 = Completato, Status 7 = Richiesta in coda.
            if response.status != 7:
                return response  # Successo (o errore non recuperabile).
            elif response.status == 7:
                # Se il server e' pieno, aspetta e riprova.
                print(f"[RETRY] Too many pending requests (status={response.status}). Waiting {sleep_time}s...")
            else:
                print(f"[RETRY] Unknown status={response.status}. Waiting {sleep_time}s...")
        except Exception as e:
            raise  # Se l'errore e' grave, lo lascia gestire al livello superiore.

        time.sleep(sleep_time)  # Attende prima del prossimo tentativo.
        retries += 1

    # Se supera il numero massimo di tentativi, lancia un errore.
    raise RuntimeError(f"Exceeded max retries ({max_retries}) for query: {query}")

# Funzione "worker" che processa un intero intervallo di date (es. un blocco di Training).
def process_range(args):
    split, start_str, end_str, output_dir = args  # Disimballa gli argomenti.

    print(f"[PID {mp.current_process().pid}] Starting date range {start_str} to {end_str}")

    # Inizializza i client per la connessione al database solare.
    client = drms.Client(email=EMAIL)
    jsoc_client = jsoc.JSOCClient()

    # Crea la cartella di output specifica per lo split (es. output/Train).
    split_path = os.path.join(output_dir, split)
    os.makedirs(split_path, exist_ok=True)

    # Converte le stringhe delle date in oggetti datetime.
    start = datetime.strptime(start_str, "%Y-%m-%d")
    end = datetime.strptime(end_str, "%Y-%m-%d")

    current = start
    # Ciclo principale che avanza di 12 minuti alla volta (cadenza strumento HMI).
    while current <= end:
        # Formatta la data nel formato richiesto da JSOC (TAI time).
        current_str = current.strftime('%Y.%m.%d_%H:%M:%S_TAI')
        # Definisce il nome del file di output .h5.
        filename = f'hmi_magnetogram_{current.strftime("%Y-%m-%d_%H-%M-%S")}.h5'
        output_h5 = os.path.join(split_path, filename)

        # Salta il download se il file esiste gia' (resume capability).
        if os.path.exists(output_h5):
            print(f"[{current}] Skipping - file already exists: {output_h5}")
            current += timedelta(minutes=12)
            continue

        # --- FASE 1: DOWNLOAD E PREPARAZIONE DATI IN MEMORIA ---
        try:
            print(f"[{current}] Requesting HMI magnetogram data...")
            # Costruisce la query per il magnetogramma (serie hmi.M_720s).
            # QUALITY<65536 filtra le immagini con errori noti.
            hmi_query = f'hmi.M_720s[{current_str}][? (QUALITY<65536) ?]'
            hmi_response = export_with_retries(client, hmi_query)

            print(f"[{current}] Creating SunPy map from FITS URL...")
            # Scarica il FITS dall'URL e crea una mappa SunPy.
            hmi_map = sunpy.map.Map(hmi_response.urls['url'][0])
            data = hmi_map.data  # Estrae la matrice numerica (immagine).
            metadata = hmi_map.meta  # Estrae l'header FITS (metadati immagine).

            print(f"[{current}] Requesting HARP metadata...")
            # Cerca i metadati delle regioni attive (HARP) per quello stesso istante.
            harp_response = jsoc_client.search(
                jsoc.attrs.Time(current_str, current_str),
                jsoc.attrs.Series('hmi.Mharp_720s'),
            )

        except Exception as e:
            # In caso di errore di rete o dati mancanti, stampa l'errore e salta al prossimo step.
            print(f"[{current}] Error during data fetch or parsing: {e}")
            current += timedelta(minutes=12)
            continue

        # --- FASE 2: SCRITTURA SICURA SU DISCO ---
        try:
            # Usa la classe SafeH5File per garantire l'integrita' del file.
            with SafeH5File(output_h5, 'w') as f:
                # Salva la matrice del magnetogramma compressa con GZIP (livello 9).
                f.create_dataset('magnetogram/data', data=data, compression="gzip", compression_opts=9)

                # Crea un gruppo per i metadati del magnetogramma.
                meta_group = f.create_group('magnetogram/metadata')
                for key in metadata:
                    value = metadata.get(key)
                    try:
                        # Prova a salvare il valore direttamente.
                        meta_group.attrs[key] = value
                    except TypeError:
                        # Se il tipo non e' supportato da HDF5, lo converte in stringa.
                        meta_group.attrs[key] = str(value)

                # Crea un gruppo per i metadati HARP (Regioni Attive).
                harp_group = f.create_group('harp/metadata')
                for record in harp_response:
                    harpnum = record["HARPNUM"]
                    harp_id = f"HARP_{harpnum}"
                    # Crea un sottogruppo per ogni HARP identificata.
                    harp_entry = harp_group.create_group(harp_id)

                    # Salva tutte le colonne della risposta HARP come attributi.
                    for key in harp_response.columns:
                        value = record[key]
                        try:
                            harp_entry.attrs[key] = value
                        except TypeError:
                            harp_entry.attrs[key] = str(value)

            print(f"[{current}] File saved: {output_h5}")

        except Exception as e:
            print(f"[{current}] Error saving file: {e}")
            # Pulizia extra nel caso SafeH5File non avesse intercettato tutto.
            if os.path.exists(output_h5):
                os.remove(output_h5)

        # Avanza l'iteratore temporale di 12 minuti.
        current += timedelta(minutes=12)

    print(f"[PID {mp.current_process().pid}] Finished processing range {start_str} to {end_str}")

# Funzione principale che orchestra l'esecuzione dei job.
def run_all(csv_path, output_root="output", num_workers=4):
    print(f"Reading from: {csv_path}")
    df = pd.read_csv(csv_path)

    # Crea una lista di tuple (task) da assegnare ai worker.
    tasks = [(row["Split"], row["Start"], row["End"], output_root) for _, row in df.iterrows()]
    print(f"Launching {len(tasks)} tasks with {num_workers} worker(s)...")

    # Codice predisposto per il multiprocessing (attualmente esegue in sequenza per debug).
    # with mp.Pool(processes=num_workers) as pool:
    #     pool.map(process_range, tasks)
    
    # Esegue i task sequenzialmente nel processo corrente.
    for task in tasks: process_range(task)

    print("All tasks completed.")

if __name__ == '__main__':
    # Punto di ingresso dello script.
    run_all('solar_cycle_splits.csv', output_root="/projects/data/physics/data", num_workers=8)
\end{lstlisting}

\section{Modifiche a train.py}
\begin{lstlisting}[language=Python, caption={Importazione in train.py}, label={lst:train_import}]
# Importa la classe personalizzata per la gestione dei dataset in formato HDF5.
from utils.dataset_h5 import DatasetH5
\end{lstlisting}

\begin{lstlisting}[language=Python, caption={Funzione Collate per il Training}, label={lst:train_collate}]
# Funzione custom per raggruppare campioni in un batch. Aggiunge l'indice del batch a ogni etichetta per l'associazione.
def h5_collate_fn(batch):
    # Separa gli elementi del batch (immagini, etichette, percorsi, dimensioni)
    imgs, labels, paths, shapes = zip(*batch)

    batched_labels = [] # Lista per accumulare le etichette indicizzate

    # Itera su ogni campione (immagine + etichette) nel batch
    # 'i' sara' l'indice dell'immagine nel batch (0, 1, 2, ...)
    for i, label in enumerate(labels):

        # Processa solo se l'immagine ha almeno un'etichetta
        if label.shape[0] > 0:

            # Crea un tensore riempito con l'indice (i) dell'immagine
            # Avra' la stessa n. di righe delle etichette di questa immagine
            batch_idx = torch.full((label.shape[0], 1), i,
                                   device=imgs[0].device)

            # Concatena l'indice (colonna 0) alle etichette [classe, x, y, w, h]
            label_with_batch_idx = torch.cat((batch_idx,
                                           label.to(imgs[0].device)), 1)

            # Aggiunge il nuovo tensore [i, classe, x, y, w, h] alla lista
            batched_labels.append(label_with_batch_idx)

    # Se sono state trovate etichette in questo batch...
    if len(batched_labels) > 0:
        # ...le unisce tutte in un unico tensore [N_tot_labels, 6]
        targets = torch.cat(batched_labels, 0)
    else:
        # ...altrimenti crea un tensore vuoto (con 6 colonne) per coerenza
        targets = torch.empty(0, 6, device=imgs[0].device)

    # Restituisce le immagini (stackate in un unico tensore batch)
    # e il tensore unico delle etichette (targets)
    return torch.stack(imgs, 0), targets, paths, shapes
\end{lstlisting}

\begin{lstlisting}[language=Python, caption={Lettura configurazione .yaml}, label={lst:train_config}]
# Apre e legge il file di configurazione del dataset (es. harp.yaml)
with open(opt.data) as f:
    data_dict = yaml.load(f, Loader=yaml.SafeLoader)

# Controlla se la chiave 'is_h5' esiste e ha valore True; altrimenti, imposta False
is_h5_dataset = data_dict.get('is_h5', False)
\end{lstlisting}

\begin{lstlisting}[language=Python, caption={Selezione dinamica del DataLoader}, label={lst:train_loader_choice}]
# Logica condizionale per la selezione dinamica del data loader

# Controlla il flag booleano letto dal file .yaml
if is_h5_dataset:
    logger.info("Utilizzo del DataLoader custom per dataset .h5")

    # Crea un'istanza del dataset personalizzato
    dataset = DatasetH5(path=train_path, img_size=imgsz)

    # Imposta il sampler (necessario per il training distribuito)
    sampler = torch.utils.data.distributed.DistributedSampler(dataset) if rank != -1 else None

    # Crea il DataLoader di PyTorch usando la collate_fn personalizzata
    dataloader = torch.utils.data.DataLoader(dataset,
                                            batch_size=batch_size,
                                            shuffle=sampler is None and not opt.rect,
                                            num_workers=opt.workers,
                                            sampler=sampler,
                                            pin_memory=True,
                                            collate_fn=h5_collate_fn)
else:
    # Logica originale di YOLOv7 per dataset standard
    dataloader, dataset = create_dataloader(train_path, imgsz, batch_size, gs, opt,
                                            hyp=hyp, augment=True, cache=opt.cache_images, rect=opt.rect, rank=rank,
                                            world_size=opt.world_size, workers=opt.workers,
                                            image_weights=opt.image_weights, quad=opt.quad, prefix=colorstr('train: '))
\end{lstlisting}

\begin{lstlisting}[language=Python, caption={Normalizzazione condizionale nel training}, label={lst:train_norm}]
if is_h5_dataset:
    # Per i dati H5, i valori sono gia' normalizzati [0,1].
    imgs = imgs.to(device, non_blocking=True).float()
else:
    # Per le immagini normali, mantengo la logica originale.
    imgs = imgs.to(device, non_blocking=True).float()/255.0
\end{lstlisting}

\section{Modifiche a test.py}
\begin{lstlisting}[language=Python, caption={Importazioni in test.py}, label={lst:test_import}]
# Importa la classe personalizzata per la gestione dei dataset in formato HDF5.
from utils.dataset_h5 import DatasetH5

# Importa la funzione di default di PyTorch per l'assemblaggio dei batch.
from torch.utils.data.dataloader import default_collate
\end{lstlisting}

\begin{lstlisting}[language=Python, caption={Funzione Collate semplificata per il Test}, label={lst:test_collate}]
# Funzione custom per raggruppare i dati provenienti da DatasetH5 in un batch.
def h5_collate_fn(batch):
# Delega l'assemblaggio del batch alla funzione di default di PyTorch,  che impila automaticamente i campioni in un unico tensore.

    return default_collate(batch)
\end{lstlisting}

\begin{lstlisting}[language=Python, caption={Nuovo parametro nella funzione test}, label={lst:test_param}]
def test(data,
         ...,
         is_magnetogram=False):
\end{lstlisting}

\begin{lstlisting}[language=Python, caption={Creazione Dataloader in test.py}, label={lst:test_loader_choice}]
# Legge il flag 'is_h5' dal dizionario 'data' (caricato dal file .yaml).
# Se la chiave non esiste, imposta 'False' come valore di default.
is_h5_dataset = data.get('is_h5', False)

# Controlla se il dataset e' di tipo H5.
if is_h5_dataset:
    # Stampa un messaggio informativo nel log.
    print("Utilizzo del DataLoader custom per dataset .h5")
    
    # Istanzia la classe DatasetH5 personalizzata.
    # 'data[task]' contiene il percorso alla cartella dei dati (es. 'val').
    dataset = DatasetH5(path=data[task], img_size=imgsz)
    
    # Crea un DataLoader standard di PyTorch utilizzando il dataset H5.
    dataloader = torch.utils.data.DataLoader(dataset,
                                             # Imposta la dimensione del batch.
                                             batch_size=batch_size,
                                             # Disattiva lo 'shuffle' (mescolamento) per la validazione/test.
                                             shuffle=False,
                                             # Imposta il numero di processi paralleli per caricare i dati.
                                             num_workers=8, 
                                             # Abilita il 'pinning' della memoria per trasferimenti piu' veloci alla GPU.
                                             pin_memory=True,
                                             # Specifica la funzione custom per assemblare i campioni in un batch.
                                             collate_fn=h5_collate_fn)
# Se il dataset non e' di tipo H5...
else:
    # ...esegue la logica originale di YOLOv7.
    # Chiama la funzione 'create_dataloader' standard del framework.
    dataloader = create_dataloader(data[task], imgsz, batch_size, gs, opt, pad=0.5, rect=True,
                                   prefix=colorstr(f'{task}: '))[0]
\end{lstlisting}

\begin{lstlisting}[language=Python, caption={Normalizzazione condizionale nel test}, label={lst:test_norm}]
# Salta la divisione se e' un magnetogramma
    if not is_magnetogram:
        img /= 255.0
\end{lstlisting}

\begin{lstlisting}[language=Python, caption={Adattamento scale\_coords}, label={lst:test_coords}]
# ... all'interno del ciclo sulle predizioni ...

# 1. Riscalare le coordinate delle PREDIZIONI
# La chiamata originale (shapes[si][0]) e' stata modificata in 'shapes[si]'
scale_coords(img[si].shape[1:], predn[:, :4], shapes[si])  # native-space pred

# ... all'interno del blocco 'if nl:' (se ci sono etichette reali) ...

# 2. Riscalare le coordinate delle ETICHETTE REALI (target)
# Anche qui, la chiamata e' stata adattata a 'shapes[si]'
tbox = xywh2xyxy(labels[:, 1:5])
scale_coords(img[si].shape[1:], tbox, shapes[si])  # native-space labels
\end{lstlisting}

\begin{lstlisting}[language=Python, caption={Chiamata al thread di plotting}, label={lst:test_plot_call}]
# Avvia un thread separato per la funzione 'plot_images' (per non bloccare il loop principale).
# 'args' passa gli argomenti posizionali (immagine, etichette, percorso, nome file, nomi classi).
# 'kwargs' (key-word arguments) passa un dizionario:
#   'is_magnetogram' viene impostato con il valore del flag 'is_magnetogram'.
# Questo permette a 'plot_images' di sapere che tipo di immagine sta visualizzando.
Thread(target=plot_images, args=(img, targets, paths, f, names), 
       kwargs={'is_magnetogram': is_magnetogram}, 
       daemon=True).start()
\end{lstlisting}

\section{Modifiche a plots.py}
\begin{lstlisting}[language=Python, caption={Firma funzione plot\_images}, label={lst:plot_sig}]
# La firma della funzione originale terminava con 'max_subplots=16'.
# E' stato aggiunto il nuovo argomento 'is_magnetogram=False' alla fine.
def plot_images(images, targets, paths=None, fname='images.jpg', names=None, 
                max_size=640, max_subplots=16, is_magnetogram=False):
    # Il resto del corpo della funzione...
\end{lstlisting}

\begin{lstlisting}[language=Python, caption={Check normalizzazione plot}, label={lst:plot_norm}]
# ... (codice precedente) ...
    if isinstance(targets, torch.Tensor):
        targets = targets.cpu().numpy()

    # La riga originale 'images *= 255' e' stata resa condizionale.
    # Si esegue solo se NON e' un magnetogramma E se i valori sono normalizzati [0,1].
    if not is_magnetogram and np.max(images[0]) <= 1:
        images *= 255
    
    tl = 3  # line thickness
    # ... (codice successivo) ...
\end{lstlisting}

\begin{lstlisting}[language=Python, caption={Logica di rendering magnetogrammi}, label={lst:plot_render}]
# ... (dentro il ciclo for i, img in enumerate(images)) ...
        
        # Inizia la logica condizionale basata sul flag
        if is_magnetogram:
            # --- Blocco personalizzato per magnetogrammi ---
            
            # 1. De-normalizza l'immagine (da [0,1] a [-1500, 1500])
            #    (img[0] seleziona il singolo canale)
            magnetogram_data = img[0] * 3000 - 1500
        
            # 2. Imposta i limiti di contrasto per la visualizzazione
            contrast_min = -300
            contrast_max = 300
            
            # 3. Ottiene la colormap 'seismic' (Rosso-Bianco-Blu)
            cmap = plt.get_cmap('seismic')
            # 4. Normalizza i dati tra [0,1] in base al contrasto
            norm = matplotlib.colors.Normalize(vmin=contrast_min, vmax=contrast_max)
            
            # 5. Applica la colormap ai dati normalizzati
            colored_img = cmap(norm(magnetogram_data))
            # 6. Converte da RGBA [0,1] a RGB [0,255] (formato uint8)
            img_rgb = (colored_img[:, :, :3] * 255).astype(np.uint8)
            
            # 7. Capovolge l'immagine verticalmente (flip Asse X)
            img = cv2.flip(img_rgb, 0)
            
            # 8. Ridimensiona se necessario (come da logica originale)
            if scale_factor < 1:
                img = cv2.resize(img, (w, h))
        else:
            # --- Blocco Originale per immagini BGR/RGB ---
            img = img.transpose(1, 2, 0) # Converte da [C, H, W] a [H, W, C]
            if scale_factor < 1:
                img = cv2.resize(img, (w, h))

        # Disegna l'immagine (ora in formato RGB) sul mosaico
        mosaic[block_y:block_y + h, block_x:block_x + w, :] = img
        # ... (codice successivo)
\end{lstlisting}

\begin{lstlisting}[language=Python, caption={Inversione coordinate Y}, label={lst:plot_flip}]
# ... (codice per il calcolo dei box) ...
        if boxes.shape[1]:
            if boxes.max() <= 1.01:
                boxes[[0, 2]] *= w
                boxes[[1, 3]] *= h
            elif scale_factor < 1:
                boxes *= scale_factor
            
        # Aggiunta: Blocco per invertire le coordinate Y se e' un magnetogramma
        if is_magnetogram:
            # Inverte le coordinate Y (indice 1 e 3) rispetto all'altezza (h)
            # Questo compensa il 'cv2.flip(..., 0)' applicato all'immagine
            boxes[[1, 3]] = h - boxes[[1, 3]]

        # Sposta i box nella loro posizione sul mosaico (logica originale)
        boxes[[0, 2]] += block_x
        boxes[[1, 3]] += block_y
            
        # ... (codice per disegnare i box) ...
\end{lstlisting}

\section{Script di Conversione}
\begin{lstlisting}[language=Python, caption={Script preprocess\_to\_zip.py completo}, label={lst:script_preprocess}]
import h5py # Libreria per leggere e scrivere file HDF5
import numpy as np # Libreria per il calcolo numerico
import cv2 # Libreria OpenCV per l'elaborazione delle immagini
import os # Libreria per interagire con il sistema operativo
import glob # Libreria per trovare file sul disco tramite pattern
from tqdm import tqdm # Libreria per mostrare una barra di progresso
import argparse # Libreria per gestire gli argomenti passati da riga di comando
import sys # Libreria per interagire con il sistema
import zipfile # Libreria per creare e scrivere file .zip
import concurrent.futures # Libreria per la gestione del multithreading

# --- Impostazioni Globali ---
IMG_SIZE = 640 # Dimensione fissa (larghezza e altezza) delle immagini di output
CLIP_MIN, CLIP_MAX = -1500, 1500 # Valori minimi e massimi per il clipping dei dati scientifici
CLASS_ID = 0 # ID di classe fisso per le etichette (abbiamo solo "regioni attive")
# Numero di thread "lavoratori" da usare per processare i file H5 in parallelo
N_WORKERS = 32 
# ---

def process_file_h5(h5_path):
    """
    Processa un singolo file H5.
    Questa funzione legge i dati, estrae le etichette, processa l'immagine,
    e restituisce i dati pronti per essere scritti nello zip.
    E' progettata per essere eseguita in un thread separato.
    """
    
    # Estrae il nome base del file (es. 'M_720s_20101210_000000_TAI_20101210_001159_TAI_01300')
    base_name = os.path.splitext(os.path.basename(h5_path))[0]
    # Definisce il percorso dell'immagine DENTRO l'archivio .zip
    img_arcname = os.path.join('images', f"{base_name}.jpg")
    # Definisce il percorso del file di etichette DENTRO l'archivio .zip
    label_arcname = os.path.join('labels', f"{base_name}.txt")

    try:
        # Apre il file H5 in modalita' lettura ('r')
        with h5py.File(h5_path, 'r') as f:
            
            # --- 1. Estrai Dati Immagine ---
            # Accede al dataset del magnetogramma
            magnetogram_data = f['magnetogram/data']
            # Legge le dimensioni originali (altezza, larghezza)
            orig_h, orig_w = magnetogram_data.shape
            # Carica l'intero array dei dati in memoria
            data = magnetogram_data[:]

            # --- 2. Estrai Etichette (Bounding Box) ---
            # Accede al gruppo dei metadati
            harp_group = f['harp/metadata']
            # Lista per contenere le etichette di QUESTA immagine
            image_labels = []
            
            # Itera su ogni regione attiva (HARP) trovata nei metadati
            for harp_id in harp_group:
                # Estrae gli attributi della regione corrente
                harp_attrs = harp_group[harp_id].attrs
                
                # Lista delle chiavi necessarie per definire un box
                required_keys = ['CRPIX1', 'CRPIX2', 'CRSIZE1', 'CRSIZE2']
                # Controlla se tutte le chiavi necessarie sono presenti
                if not all(key in harp_attrs for key in required_keys):
                    continue # Se ne manca una, salta questa etichetta

                # Converte le dimensioni (in pixel) in float
                w_abs = float(harp_attrs['CRSIZE1'])
                h_abs = float(harp_attrs['CRSIZE2'])

                # Validazione: scarta etichette con dimensioni non positive
                if w_abs <= 0 or h_abs <= 0:
                    continue # Salta questa etichetta

                # Calcola le coordinate normalizzate in formato YOLO (centro_x, centro_y, w, h)
                x_center_norm = (float(harp_attrs['CRPIX1']) + w_abs / 2) / orig_w
                y_center_norm = (float(harp_attrs['CRPIX2']) + h_abs / 2) / orig_h
                width_norm = w_abs / orig_w
                height_norm = h_abs / orig_h

                # Validazione: scarta etichette il cui centro e' fuori dall'immagine
                if not (0.0 < x_center_norm < 1.0 and 0.0 < y_center_norm < 1.0):
                    continue # Salta questa etichetta
                
                # Aggiunge l'etichetta valida alla lista
                image_labels.append([CLASS_ID, x_center_norm, y_center_norm, width_norm, height_norm])

            # --- 3. Processa Immagine (come in DatasetH5) ---
            # Pulizia: sostituisce NaN e Infinito con 0.0
            if np.isnan(data).any() or np.isinf(data).any():
                data = np.nan_to_num(data, nan=0.0, posinf=0.0, neginf=0.0)
            
            # Clipping: "taglia" i valori all'intervallo definito
            clipped_data = np.clip(data, CLIP_MIN, CLIP_MAX)
            # Normalizzazione Min-Max: porta i valori nell'intervallo [0.0, 1.0]
            normalized_data = (clipped_data - CLIP_MIN) / (CLIP_MAX - CLIP_MIN)
            # Ridimensionamento: porta l'immagine alla dimensione 640x640
            resized_image = cv2.resize(normalized_data, (IMG_SIZE, IMG_SIZE), interpolation=cv2.INTER_LINEAR)
            
            # Conversione 8-bit: "appiattisce" i dati float [0,1] in interi [0,255]
            image_uint8 = (resized_image * 255.0).astype(np.uint8)
            # Conversione 3 Canali: duplica il canale unico per creare un'immagine RGB
            image_rgb = np.stack([image_uint8] * 3, axis=-1)
            
            # Codifica in memoria: converte l'array NumPy in un formato JPG (binario)
            is_success, img_buffer = cv2.imencode('.jpg', image_rgb)
            if not is_success:
                # Se la codifica fallisce, solleva un'eccezione
                raise Exception("Impossibile codificare l'immagine in JPG.")
            
            # --- 4. Prepara Etichette (Formato TXT) ---
            # Crea una lista di stringhe, una per ogni etichetta
            label_lines = [f"{lbl[0]} {lbl[1]} {lbl[2]} {lbl[3]} {lbl[4]}" for lbl in image_labels]
            # Unisce le stringhe con un "a capo", pronto per essere scritto su file
            label_content = "\n".join(label_lines)
            
            # Restituisce tutti i dati necessari al thread principale per la scrittura
            return (img_arcname, img_buffer.tobytes(), label_arcname, label_content)

    except Exception as e:
        # Gestione degli errori (es. file H5 corrotto)
        print(f"\nATTENZIONE: Fallimento nel processare {h5_path}: {e}")
        # Crea un'immagine nera (placeholder)
        placeholder_img = np.zeros((IMG_SIZE, IMG_SIZE, 3), dtype=np.uint8)
        # Codifica l'immagine nera
        is_success, img_buffer = cv2.imencode('.jpg', placeholder_img)
        # Restituisce l'immagine nera e un file etichette vuoto
        return (img_arcname, img_buffer.tobytes(), label_arcname, "")


def main():
    """
    Funzione principale che orchestra il processo.
    Gestisce gli argomenti da riga di comando, trova i file,
    avvia il ThreadPool ed esegue la scrittura del file .zip.
    """
    
    # Configura il parser per gli argomenti da riga di comando
    parser = argparse.ArgumentParser(description="Converte il dataset H5 in un singolo file .zip in formato YOLO (multithread).")
    parser.add_argument('--source-dir', type=str, required=True, help="Cartella locale contenente i file .h5")
    parser.add_argument('--zip-file', type=str, required=True, help="Percorso del file .zip di output")
    parser.add_argument('--workers', type=int, default=N_WORKERS, help="Numero di thread 'produttori'")
    args = parser.parse_args() # Legge gli argomenti forniti dall'utente

    # Trova tutti i file .h5 nella cartella sorgente (anche sottocartelle)
    h5_files = sorted(glob.glob(os.path.join(args.source_dir, '**', '*.h5'), recursive=True))
    if not h5_files:
        # Se non trova file, stampa un errore ed esce
        print(f"Errore: Nessun file .h5 trovato in {args.source_dir}")
        sys.exit(1)
        
    print(f"Trovati {len(h5_files)} file .h5. Avvio della conversione in '{args.zip_file}'...")
    print(f"Uso di {args.workers} thread lavoratori.")

    # Crea e gestisce un pool di thread (max 'args.workers' thread attivi contemporaneamente)
    with concurrent.futures.ThreadPoolExecutor(max_workers=args.workers) as executor:
        
        # Apre il file .zip in modalita' scrittura ('w') con compressione
        with zipfile.ZipFile(args.zip_file, 'w', compression=zipfile.ZIP_DEFLATED) as zf:
            
            # Sottomette tutti i lavori (chiama 'process_file_h5' per ogni file)
            # 'futures' e' una lista di "promesse" di risultati futuri
            futures = [executor.submit(process_file_h5, h5_path) for h5_path in h5_files]
            
            # Il thread principale (questo) ora itera sui risultati man mano che
            # i thread "lavoratori" li completano (non in ordine di invio)
            # 'tqdm' crea una barra di progresso per questa iterazione
            for future in tqdm(concurrent.futures.as_completed(futures), total=len(h5_files), desc="Conversione in corso"):
                
                # Ottiene il risultato dal thread completato
                result = future.result()
                
                if result:
                    # Estrae i dati restituiti dalla funzione
                    img_arcname, img_buffer, label_arcname, label_content = result
                    
                    # Scrive l'immagine (binaria) nel file .zip
                    zf.writestr(img_arcname, img_buffer)
                    # Scrive le etichette (testo) nel file .zip
                    zf.writestr(label_arcname, label_content)

    print("\nConversione completata.")
    print(f"Il file '{args.zip_file}' e' stato creato con successo.")

# Questo blocco assicura che la funzione 'main()' sia eseguita
# solo quando lo script e' avviato direttamente (non se importato)
if __name__ == "__main__":
    main()
\end{lstlisting}

\section{Script di Post-Processing}
\begin{lstlisting}[language=Python, caption={Script convert.py per il mascheramento}, label={lst:script_convert}]
# Importa la libreria OpenCV per l'elaborazione delle immagini
import cv2
# Importa la libreria NumPy per il calcolo numerico e la gestione degli array
import numpy as np
# Importa la libreria per interagire con il sistema operativo
import os
# Importa la libreria per cercare file sul disco che corrispondono a un pattern
import glob

# --- Impostazioni ---
# Definisce il percorso da cui leggere le immagini originali (con sfondo grigio)
CARTELLA_INPUT = 'Validation/images_original'
# Definisce il percorso in cui salvare le immagini pulite (con sfondo nero)
CARTELLA_OUTPUT = 'Validation/images'

# Percentuale del raggio. 1.0 = fino al bordo.
# 0.96 taglia via i bordi rumorosi/sfumati e lo sfondo.
FATTORE_RAGGIO = 0.96 
# --------------------

# Crea la cartella di output se non esiste
# Controlla se la cartella di output non esiste gia'
if not os.path.exists(CARTELLA_OUTPUT):
    # Crea la cartella di output (e tutte le cartelle intermedie necessarie)
    os.makedirs(CARTELLA_OUTPUT)
    # Stampa un messaggio di conferma della creazione
    print(f"Cartella '{CARTELLA_OUTPUT}' creata.")

# Cerca tutti i file immagine (anche nelle sottocartelle)
# Definisce una tupla di estensioni di file immagine da cercare
tipi_file = ('*.jpg', '*.jpeg', '*.png', '*.bmp', '*.tif')
# Inizializza una lista vuota per contenere i percorsi dei file trovati
file_immagini = []
# Avvia un ciclo per ogni estensione definita in 'tipi_file'
for tipo in tipi_file:
    # Cerca (ricorsivamente, '**') i file che corrispondono al pattern e li aggiunge alla lista
    file_immagini.extend(glob.glob(os.path.join(CARTELLA_INPUT, '**', tipo), recursive=True))

# Controlla se sono stati trovati file
# Controlla se la lista 'file_immagini' e' vuota (nessun file trovato)
if not file_immagini:
    # Stampa un messaggio di avviso se non sono state trovate immagini
    print(f"Nessuna immagine trovata nella cartella '{CARTELLA_INPUT}'.")
    # Stampa un suggerimento per l'utente
    print("Assicurati di aver creato una cartella 'input' e di averci messo le tue immagini.")
# Blocco eseguito se almeno un'immagine e' stata trovata
else:
    # Stampa il numero di immagini trovate e avvia l'elaborazione
    print(f"Trovate {len(file_immagini)} immagini. Inizio elaborazione...")

# Elabora ogni immagine
# Avvia un ciclo che itera su ogni percorso di file trovato
for percorso_immagine in file_immagini:
    # Carica l'immagine originale
    # Legge il file immagine dal disco e lo carica in un array NumPy
    img_originale = cv2.imread(percorso_immagine)
    
    # Controlla se il caricamento dell'immagine e' fallito (es. file corrotto)
    if img_originale is None:
        # Stampa un messaggio di errore specificando il file
        print(f"Errore: Impossibile caricare l'immagine {percorso_immagine}")
        # Interrompe questa iterazione del ciclo e passa al file successivo
        continue
        
    # Ottieni le dimensioni dell'immagine
    # Estrae l'altezza ('h') e la larghezza ('w') dalle dimensioni dell'array immagine
    h, w = img_originale.shape[:2]

    # Calcola il centro e il raggio
    # Calcola la coordinata X del centro dell'immagine (divisione intera)
    centro_x = w // 2
    # Calcola la coordinata Y del centro dell'immagine (divisione intera)
    centro_y = h // 2
    
    # Calcola il raggio basandoti sulla dimensione piu' piccola
    # Trova il raggio massimo possibile (basato sul lato piu' corto dal centro)
    raggio_base = min(centro_x, centro_y)
    # e applica il fattore per escludere i bordi (es. 0.96) e converte in intero
    raggio = int(raggio_base * FATTORE_RAGGIO)

    # 1. Crea una maschera completamente nera
    # (della stessa dimensione e tipo dell'originale)
    # Crea un array NumPy pieno di zeri (nero) con le stesse dimensioni di 'img_originale'
    maschera = np.zeros_like(img_originale)

    # 2. Disegna un cerchio pieno bianco sulla maschera
    # Questo cerchio rappresenta l'area che vogliamo conservare
    # Disegna un cerchio bianco pieno sulla maschera
    cv2.circle(maschera, (centro_x, centro_y), raggio, (255, 255, 255), thickness=cv2.FILLED)

    # 3. Applica la maschera all'immagine originale
    # cv2.bitwise_and mantiene solo i pixel dove entrambe
    # le immagini (originale e maschera) sono non-nere.
    # Esegue un'operazione AND bit-per-bit. I pixel fuori dal cerchio (dove la maschera e' 0) diventano 0.
    risultato = cv2.bitwise_and(img_originale, maschera)

    # Costruisci il percorso di output mantenendo la struttura
    # Calcola il percorso relativo del file (es. 'sottocartella/img.jpg')
    percorso_relativo = os.path.relpath(percorso_immagine, CARTELLA_INPUT)
    # Ricostruisce il percorso di destinazione nella cartella di output
    percorso_output = os.path.join(CARTELLA_OUTPUT, percorso_relativo)
    
    # Estrae il percorso della cartella di destinazione (es. 'Validation/images/sottocartella')
    cartella_destinazione = os.path.dirname(percorso_output)
    # Controlla se la cartella di destinazione (per le sottocartelle) non esiste
    if not os.path.exists(cartella_destinazione):
        # Crea la sottocartella di destinazione se necessario
        os.makedirs(cartella_destinazione)

    # Salva l'immagine modificata
    # Salva l'array 'risultato' (l'immagine mascherata) sul disco nel percorso di output
    cv2.imwrite(percorso_output, risultato)
    
# Stampa un messaggio finale al termine di tutti i cicli
print(f"\nElaborazione completata. Immagini salvate in '{CARTELLA_OUTPUT}'.")
\end{lstlisting}

\section{Script di Tracciamento e Analisi}

\begin{lstlisting}[language=Python, caption={Script di tracciamento (track.py)}, label={lst:track_script}]
import argparse  # Gestione degli argomenti da riga di comando
import os  # Interazione con il sistema operativo
from typing import List  # Per il type hinting
import subprocess  # Per eseguire comandi esterni (es. FFmpeg)
import numpy as np  # Calcolo numerico e matriciale
import torch  # Framework PyTorch per deep learning
import cv2  # OpenCV per elaborazione immagini
import h5py  # Lettura file scientifici HDF5
import glob  # Ricerca file tramite pattern
from tqdm import tqdm  # Barra di avanzamento
import norfair  # Libreria di Object Tracking
from norfair import Detection, Tracker  # Classi principali di Norfair
from models.experimental import attempt_load  # Caricamento modello YOLOv7
from utils.general import non_max_suppression  # Algoritmo NMS

# --- FUNZIONI DI UTILITA' ---
def yolo_detections_to_norfair_detections(yolo_detections: torch.tensor) -> List[Detection]:
    """Converte le rilevazioni di YOLOv7 in un formato comprensibile da Norfair."""
    norfair_detections: List[Detection] = []
    # yolo_detections ha formato [x_min, y_min, x_max, y_max, conf, class]
    for detection in yolo_detections:
        # Estrae le coordinate del bounding box
        bbox = np.array(
            [
                [detection[0].item(), detection[1].item()],
                [detection[2].item(), detection[3].item()],
            ]
        )
        # Estrae i punteggi di confidenza
        scores = np.array([detection[4].item(), detection[4].item()])
        # Crea l'oggetto Detection richiesto da Norfair
        norfair_detections.append(
            Detection(points=bbox, scores=scores, label=int(detection[5].item()))
        )
    return norfair_detections

# --- MAIN ---
# 1. DEFINIZIONE DEGLI ARGOMENTI
parser = argparse.ArgumentParser(description="Traccia le regioni attive solari.")
parser.add_argument("--input-dir", type=str, required=True, help="Percorso file .h5 o .jpg.")
parser.add_argument("--model-path", type=str, required=True, help="Percorso modello .pt.")
parser.add_argument("--img-size", type=int, default=640, help="Dimensione inferenza.")
parser.add_argument("--conf-threshold", type=float, default=0.25, help="Soglia confidenza.")
parser.add_argument("--output-dir", type=str, default="output_tracking", help="Cartella output.")
parser.add_argument("--clip-min", type=int, default=-1500, help="Min clipping H5.")
parser.add_argument("--clip-max", type=int, default=1500, help="Max clipping H5.")
args = parser.parse_args()  # Parsing degli argomenti

# 2. IMPOSTAZIONI INIZIALI
os.makedirs(args.output_dir, exist_ok=True)  # Crea cartella output se non esiste
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")  # Selezione hardware

# 3. CARICAMENTO DEL MODELLO
print("Caricamento del modello...")
# Carica i pesi del modello sul device selezionato
model = attempt_load(args.model_path, map_location=device)
model.eval()  # Imposta il modello in modalita' valutazione (inferenza)

# 4. INIZIALIZZAZIONE DI NORFAIR
# Configura il tracker basato sulla sovrapposizione spaziale (IOU)
tracker = Tracker(
    distance_function="iou",
    distance_threshold=0.7,  # Soglia di distanza per associare oggetti
)

# 5. RICERCA DEI FILE
# Cerca prima i file scientifici H5
input_files = sorted(glob.glob(os.path.join(args.input_dir, '*.h5')))
if not input_files: # Se non trova H5, cerca immagini JPG/JPEG
    input_files = sorted(glob.glob(os.path.join(args.input_dir, '*.jpg'))) + \
                  sorted(glob.glob(os.path.join(args.input_dir, '*.jpeg')))

if not input_files:
    print(f"ERRORE: Nessun file trovato in {args.input_dir}")
    exit()

# 6. CICLO DI ELABORAZIONE
for file_path in tqdm(input_files, desc="Tracking"):
    
    # --- LOGICA DI CARICAMENTO IBRIDA ---
    if file_path.endswith('.h5'): # Caso: File Scientifico
        with h5py.File(file_path, 'r') as f:
            data = f['magnetogram/data'][:]  # Legge matrice dati
        # Crea maschera per i valori mancanti (NaN)
        background_mask = np.isnan(data)
        # Sostituisce NaN con 0.0 per evitare errori numerici
        data = np.nan_to_num(data, nan=0.0, posinf=0.0, neginf=0.0)
        # Applica clipping fisico ai valori del campo magnetico
        clipped_data = np.clip(data, args.clip_min, args.clip_max)
        # Normalizza i dati nell'intervallo [0, 1]
        normalized_data = (clipped_data - args.clip_min) / (args.clip_max - args.clip_min)
        # Ridimensiona l'immagine per l'inferenza
        resized_image = cv2.resize(normalized_data, (args.img_size, args.img_size), interpolation=cv2.INTER_LINEAR)
        
        # Preparazione visualizzazione (Converte scala di grigi in BGR per OpenCV)
        display_image_gray = (resized_image * 255).astype(np.uint8)
        # Ridimensiona la maschera dello sfondo
        background_mask_resized = cv2.resize(background_mask.astype(np.uint8), (args.img_size, args.img_size), interpolation=cv2.INTER_NEAREST)
        # Imposta a nero i pixel dello sfondo usando la maschera
        display_image_gray[background_mask_resized == 1] = 0
        display_image = cv2.cvtColor(display_image_gray, cv2.COLOR_GRAY2BGR)
        # Crea il tensore input duplicando i canali (1 -> 3)
        image_tensor_np = np.stack([resized_image] * 3, axis=-1)

    else: # Caso: Immagine Standard
        img_raw = cv2.imread(file_path)  # Legge immagine
        if img_raw is None: continue  # Salta se errore lettura
        # Ridimensiona immagine
        display_image = cv2.resize(img_raw, (args.img_size, args.img_size), interpolation=cv2.INTER_LINEAR)
        # Normalizza pixel tra 0 e 1
        image_tensor_np = display_image.astype(np.float32) / 255.0

    # Conversione in Tensore PyTorch e spostamento su GPU/CPU
    # Transpone dimensioni da [H, W, C] a [C, H, W]
    image_tensor = torch.from_numpy(image_tensor_np.transpose(2, 0, 1)).float().to(device)
    image_tensor = image_tensor.unsqueeze(0)  # Aggiunge dimensione batch

    # --- Inferenza ---
    with torch.no_grad():  # Disabilita calcolo gradienti
        results = model(image_tensor, augment=False)[0]
    # Applica Non-Maximum Suppression per filtrare box sovrapposti
    results = non_max_suppression(results, conf_thres=args.conf_threshold)[0]
    
    # --- Update Tracker ---
    if results is not None and len(results) > 0:
        # Converte rilevazioni YOLO in oggetti Norfair
        detections = yolo_detections_to_norfair_detections(results)
        # Aggiorna il tracker con le nuove posizioni
        tracked_objects = tracker.update(detections=detections)
    else:
        # Aggiorna il tracker anche se non ci sono rilevazioni (per gestire oggetti persi)
        tracked_objects = tracker.update(detections=[])

    # Disegna e Salva
    # Disegna i box e gli ID tracciati sull'immagine
    norfair.draw_tracked_boxes(display_image, tracked_objects)
    # Definisce nome file output (cambia estensione in .jpg se necessario)
    output_filename = os.path.basename(file_path).replace('.h5', '.jpg')
    # Salva il frame processato su disco
    cv2.imwrite(os.path.join(args.output_dir, output_filename), display_image)

# Generazione Video con FFMPEG
print("\nGenerazione video...")
# Esegue comando esterno ffmpeg per unire i frame in un video MP4
subprocess.run([
    'ffmpeg', '-y', '-r', '10', '-pattern_type', 'glob',
    '-i', f'{args.output_dir}/*.jpg', 
    '-c:v', 'libx264', '-pix_fmt', 'yuv420p',
    os.path.join(args.output_dir, "tracking_video.mp4")
])
\end{lstlisting}

\begin{lstlisting}[language=Python, caption={Script per il calcolo delle metriche MOT (track\_metrics.py)}, label={lst:track_metrics}]
import argparse  # Gestione argomenti riga di comando
import os  # Gestione file system
import glob  # Ricerca file
import shutil  # Operazioni su file e cartelle (es. rimozione)
import numpy as np  # Calcolo numerico
import torch  # Deep Learning
import cv2  # OpenCV
import h5py  # Lettura file H5
from tqdm import tqdm  # Barra di progresso
import norfair  # Libreria Tracking
from norfair import Detection, Tracker  # Classi base
# Importazioni specifiche per il calcolo delle metriche MOT
from norfair.metrics import Accumulators, InformationFile 
from models.experimental import attempt_load  # Loading YOLO
from utils.general import non_max_suppression  # NMS

# --- 1. UTILITY: Prepara il GT personalizzato per Norfair ---
def setup_custom_mot_structure(custom_gt_path, img_list, output_base, img_w, img_h):
    """
    Legge il file ground_truth proprietario e crea la struttura di cartelle 
    standard 'MOTChallenge' richiesta dalle metriche di Norfair.
    """
    # Crea cartella 'gt' all'interno della directory temporanea
    gt_dir = os.path.join(output_base, "gt")
    os.makedirs(gt_dir, exist_ok=True)
    
    dest_gt_path = os.path.join(gt_dir, "gt.txt")
    
    print(f"Preparazione Ground Truth da: {custom_gt_path}")
    
    if not os.path.exists(custom_gt_path):
        print(f"ERRORE CRITICO: Il file GT {custom_gt_path} non esiste!")
        exit()

    # Legge il file originale e lo converte riga per riga
    with open(custom_gt_path, "r") as in_f, open(dest_gt_path, "w") as out_f:
        for line in in_f:
            parts = line.strip().split(',')
            if len(parts) < 6: continue
            
            try:
                # Parsing dei dati: frame, id, x, y, w, h
                frame = int(parts[0])
                obj_id = int(parts[1])
                x = float(parts[2])
                y = float(parts[3])
                w = float(parts[4])
                h = float(parts[5])
                
                # Scrive nel formato standard MOT:
                # frame, id, left, top, width, height, conf(1), -1, -1, -1
                mot_line = f"{frame},{obj_id},{x:.2f},{y:.2f},{w:.2f},{h:.2f},1,-1,-1,-1\n"
                out_f.write(mot_line)
            except ValueError:
                continue

    # Creazione del file seqinfo.ini (fondamentale per le metriche)
    seq_len = len(img_list)
    seqinfo_path = os.path.join(output_base, "seqinfo.ini")
    with open(seqinfo_path, "w") as f:
        f.write("[Sequence]\n")
        f.write(f"name={os.path.basename(output_base)}\n")
        f.write(f"imDir=img1\n")
        f.write(f"frameRate=30\n")
        f.write(f"seqLength={seq_len}\n")
        f.write(f"imWidth={img_w}\n")
        f.write(f"imHeight={img_h}\n")
        f.write(f"imExt=.jpg\n")

# --- 2. CONVERSIONE YOLO -> NORFAIR ---
def yolo_to_norfair(yolo_preds):
    """Converte i tensori di output di YOLO in oggetti Detection di Norfair."""
    norfair_detections = []
    for det in yolo_preds:
        # Estrae bbox [x_min, y_min, x_max, y_max]
        bbox = np.array([
            [det[0].item(), det[1].item()],
            [det[2].item(), det[3].item()]
        ])
        scores = np.array([det[4].item(), det[4].item()])
        label = int(det[5].item())
        norfair_detections.append(Detection(points=bbox, scores=scores, label=label))
    return norfair_detections

# --- MAIN ---
if __name__ == "__main__":
    # Configurazione argomenti
    parser = argparse.ArgumentParser()
    parser.add_argument("--source", type=str, required=True, help="Cartella immagini")
    parser.add_argument("--gt-file", type=str, required=True, help="File GT proprietario")
    parser.add_argument("--model-path", type=str, required=True, help="Modello .pt")
    parser.add_argument("--img-size", type=int, default=640)
    parser.add_argument("--conf-thres", type=float, default=0.25)
    args = parser.parse_args()

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # 1. Carica Modello (Gestione JIT/Torchscript)
    print(f"Caricamento modello: {args.model_path}")
    try:
        model = attempt_load(args.model_path, map_location=device)
    except (AttributeError, RuntimeError):
        # Fallback per modelli esportati in TorchScript
        print("Rilevato modello JIT/Traced. Uso torch.jit.load...")
        model = torch.jit.load(args.model_path, map_location=device)
    model.eval()

    # 2. Lista File
    input_files = sorted(glob.glob(os.path.join(args.source, '*.*')))
    # Filtra solo le estensioni supportate
    input_files = [f for f in input_files if f.endswith(('.jpg', '.jpeg', '.png', '.h5'))]
    if not input_files:
        print("ERRORE: Nessun file trovato.")
        exit()

    # Prepara Cartella temporanea per le metriche
    TEMP_MOT_DIR = "temp_metrics_env"
    if os.path.exists(TEMP_MOT_DIR): shutil.rmtree(TEMP_MOT_DIR)
    os.makedirs(TEMP_MOT_DIR, exist_ok=True)

    # 3. Setup Ambiente Metriche
    # Converte il GT e crea seqinfo.ini
    setup_custom_mot_structure(args.gt_file, input_files, TEMP_MOT_DIR, args.img_size, args.img_size)

    # 4. Inizializza Tracker
    tracker = Tracker(
        distance_function="iou", 
        distance_threshold=0.7,
        detection_threshold=args.conf_thres
    )
    
    # Inizializza oggetti per calcolo metriche
    print("Inizializzazione Metriche...")
    seqinfo_path = os.path.join(TEMP_MOT_DIR, "seqinfo.ini")
    
    try:
        # Crea oggetto InformationFile leggendo il file .ini generato
        info_file = InformationFile(file_path=seqinfo_path)
        
        # Crea accumulatore di statistiche
        acc = Accumulators()
        acc.create_accumulator(input_path=TEMP_MOT_DIR, information_file=info_file)
        
    except Exception as e:
        print(f"Errore inizializzazione Norfair: {e}")
        exit()

    print(f"\nAvvio Tracking su {len(input_files)} frames...")
    
    # LOOP PRINCIPALE
    for i, path in enumerate(tqdm(input_files)):
        # Caricamento e Pre-processing Ibrido
        if path.endswith('.h5'):
            with h5py.File(path, 'r') as f: data = np.nan_to_num(f['magnetogram/data'][:])
            # Normalizzazione scientifica [-1500, 1500] -> [0, 1]
            norm = (np.clip(data, -1500, 1500) + 1500) / 3000
            img0 = cv2.resize(norm, (args.img_size, args.img_size))
            img_tensor = torch.from_numpy(np.stack([img0]*3, axis=-1)).float()
        else:
            img0 = cv2.imread(path)
            if img0 is None: continue
            img0 = cv2.resize(img0, (args.img_size, args.img_size))
            img_tensor = torch.from_numpy(img0 / 255.0).float()

        # Preparazione tensore GPU
        img_tensor = img_tensor.permute(2, 0, 1).unsqueeze(0).to(device)

        # Inferenza
        with torch.no_grad():
            try:
                pred = model(img_tensor, augment=False)[0]
            except:
                pred = model(img_tensor)[0] # Fallback JIT
        
        # Filtraggio NMS
        pred = non_max_suppression(pred, args.conf_thres)[0]

        # Aggiornamento Tracker
        if pred is not None and len(pred) > 0:
            dets = yolo_to_norfair(pred)
            tracked_objs = tracker.update(detections=dets)
        else:
            tracked_objs = tracker.update(detections=[])

        # Aggiornamento Metriche
        # Confronta le predizioni del tracker con il GT per questo frame
        acc.update(predictions=tracked_objs)

    # 5. Calcolo Risultati Finali
    print("\n" + "="*40)
    print(" RISULTATI FINALI ")
    print("="*40)
    
    try:
        # Calcola le metriche (MOTA, IDF1, ecc.)
        metrics = acc.compute_metrics()
        print(metrics)
        
        # Salva report su file
        os.makedirs("risultati_finali", exist_ok=True)
        acc.save_metrics(save_path="risultati_finali", file_name="report_metriche_complete.txt")
        print("\nReport salvato in: risultati_finali/report_metriche.txt")
        
    except Exception as e:
        print(f"\nERRORE calcolo metriche: {e}")
\end{lstlisting}