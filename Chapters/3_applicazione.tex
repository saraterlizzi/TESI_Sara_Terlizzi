\chapter{Applicazione realizzata}
\label{cap:3}
Nel presente capitolo, è descritta dettagliatamente l'intera architettura software del sistema sviluppato, focalizzandosi sulle strategie metodologiche adottate per trasformare un framework di Object Detection generico in uno strumento idoneo all'utilizzo di dati astrofisici.\\
L'analisi si concentra sull'adattamento strutturale del modello YOLOv7. In particolare, è illustrato come esso sia stato esteso per processare i magnetogrammi solari, integrando moduli specifici per la gestione del formato scientifico e per il tracciamento temporale delle predizioni. \\
Successivamente, viene discusso un approccio alternativo, basato sull'adattamento del dato piuttosto che del modello: un processo di pre-elaborazione che converte i dati scientifici in un formato standard, rendendoli compatibili con l'architettura originale, senza alterarne la struttura interna.

\section{Adattamento del modello YOLOv7}
\label{sec:adattamento_yolo}
L'utilizzo di un modello di Object Detection generico come YOLOv7 nel dominio dell'astrofisica solare pone una sfida architetturale notevole: l'incompatibilità del framework con i formati di dati scientifici.\\
In questa sezione, è illustrata la metodologia adottata per estendere le capacità di input del modello, abilitando il sistema all'interpretazione dei dati HARP. L'obiettivo è stato quello di superare i vincoli imposti dai formati di immagine tradizionali (attesi dal modello), garantendo che l'informazione fisica contenuta nei magnetogrammi venisse oreservata integralmente durante il processo di caricamento.
\subsection{Implementazione di un Data Loader per Dati Scientifici}
\label{sec:dataset_h5}
Come ampiamente anticipato, l'architettura di YOLOv7 è vincolata all'uso di immagini standard (es. \texttt{JPEG, PNG}) e annotazioni testuali esplicite. I dati HARP, al contrario, presentano una complessità strutturale data da:
\begin{itemize}
    \item \textbf{Formato contenitore:} i dati sono racchiusi da un formato \texttt{HDF5} (\texttt{.h5}), ovvero un file system gerarchico pensato per dati scientifici. All'interno del singolo file, i dati sono suddivisi in gruppi distinti e non solo come una semplice immagine.
    \item \textbf{Annotazioni implicite:} le coordinate delle bounding box non sono fornite esplicitamente, bensì è necessaria l'estrazione, partendo dagli attributi incastonati nella gerarchia del file.
\end{itemize}
Per colmare tale divario, è stato progettato ed implementato un \textbf{modulo di caricamento dati personalizzato}, che agisce come interfaccia tra il formato scientifico e il tensore di input richiesto dal modello. Il componente implementa due strategie metodologiche per garantire efficienza e stabilità: un meccanismo di \textit{Caching dei Metadati} per ottimizzare i tempi di avvio e un sistema di \textit{gestione delle eccezioni} per ignorare i file corrotti in fase di addestramento.
    \subsubsection{Fase di Inizializzazione e Caching}
    Questa fase gestisce la pre-elaborazione di tutti i metadati del dataset. Per evitare la ridondanza di operazioni costose ad ogni avvio del training, è stata implementata una logica di \textit{caching}. I passi sono i seguenti:
    \begin{enumerate}
        \item \textbf{Verifica della Cache}: Il sistema verifica preventivamente l'esistenza di strutture dati contenenti le etichette già elaborate e le dimensioni originali dell'immagine.
        \item \textbf{Caricamento Veloce (\textit{Cache Hit})}: Se i file di cache sono presenti, le informazioni sono caricate immediatamente in memoria, riducendo i tempi di inizializzazione: questo rende il processo di sperimentazione molto più agile.
        \item \textbf{Indicizzazione (\textit{Cache Miss})}: In assenza di cache, (prima esecuzione), il sistema scansiona ogni file scientifico, eseguendo le seguenti operazioni:
        \begin{enumerate}
            \item \textbf{Estrazione}: accesso alla struttura interna del file per recuperare le dimensioni del magnetogramma;
            \item \textbf{Validazione dei Metadati}: viene iterato ogni sottogruppo, ciascuno dei quali rappresenta una regione attiva, sottoponendolo a tre controlli di integrità dell'etichetta:
            \begin{enumerate}
                \item \textbf{Completezza}: verifica della presenza di tutti gli attributi necessari per la definizione delle bounding box: coordinate x e y del pixel di riferimento della regione, larghezza e altezza in pixel della regione.
                Se anche solo uno di questi attributi manca, la regione è scartata.
                \item \textbf{Consistenza Dimensionale}: scarto delle annotazioni con dimensioni nulle o negative.
                \item \textbf{Limiti (\textit{Boundary Check})}: si esegue un controllo sulle coordinate normalizzate rispetto al centro della bounding box, escludendo dal dataset le annotazioni il cui baricentro ricade esternamente ai confini dell'immagine (valori non compresi tra 0.0 e 1.0). Questo filtraggio garantisce la validità spaziale delle regioni attive, preservando tuttavia quelle parzialmente visibili ai bordi purché il loro nucleo centrale sia all'interno del campo visivo. 
            \end{enumerate} 
            \item \textbf{Memorizzazione}: le coordinate validate sono normalizzate e salvate in memoria.
        \end{enumerate}
        \item \textbf{Serializzazione}: Al termine del processo, i dati strutturati sono salvati su disco per essere riutilizzati nelle esecuzioni future.
    \end{enumerate}
    \subsubsection{Procedura di Accesso e Trasformazione del Dato}
    Durante la fase di addestramento, il modulo è interrogato ripetutamente dai processi paralleli per caricare i singoli magnetogrammi.\\
    La procedura segue un flusso di trasformazione rigoroso:
    \begin{enumerate}
        \item \textbf{Lettura del Dato}: Il dato grezzo viene estratto dal file scientifico, con il supporto di meccanismi di controllo per gestire eventuali errori di Input/Output.
        \item \textbf{Pre-processing}: Se la lettura ha successo, il magnetogramma subisce una catena di trasformazioni per diventare un input valido per la rete neurale:
        \begin{enumerate}
            \item \textbf{Trattamento delle Anomalie Numeriche}: I dati scientifici possono contenere valori non validi come \texttt{Nan} (Not A Number) o \texttt{inf} (infinito), spesso causati da errori di misurazione o di calcolo. Non essendo valori numericamente stabili, se passati ad una rete neurale, causerebbero un fallimento nel processo di training. Per questo, è eseguita una pulizia preventiva che sostituisce le eventuali occorrenze di tali valori con il valore neutro \texttt{(0.0)}.
            \item \textbf{Clipping Dinamico}: I magnetogrammi presentano una notevole variazione di intensità tra le diverse aree, ma i valori più significativi per l'identificazione delle regioni attive si trovano in un intervallo specifico, mentre valori oltremodo alti o bassi rappresentano -nella maggior parte dei casi- rumori. Per questo motivo è necessaria un'operazione di \textit{clipping}: si "tagliano" i valori del magnetogramma, forzando tutti i pixel al di fuori a rientrare nell'intervallo predefinito, aiutando in questo modo il modello a concentrarsi sulle caratteristiche più significative.
            \item \textbf{Normalizzazione Min-Max}: Poiché le reti neurali apprendono più efficientemente quando i dati in input sono scalati in un intervallo piccolo, si applica una normalizzazione che riscala tutti i valori dei pixel all'interno dell'intervallo \texttt{[0,1]}, garantendo la stessa scala di valori che accelera la convergenza del training.
            \item \textbf{Ridimensionamento}: Poiché YOLOv7 richiede una dimensione di input standard (impostata a 640x640), i magnetogrammi sono ridimensionati, utilizzando un'interpolazione lineare che rappresenta un ottimo compromesso tra qualità visiva e velocità di calcolo.
            \item \textbf{Conversione a 3 Canali}: Dato che YOLOv7 accetta unicamente immagini a tre canali (RGB), è stato necessario rendere i magnetogrammi (a singolo canale) compatibili, per cui il canale unico è stato duplicato tre volte. Ciò significa che non viene aggiunta informazione, ma c'è un semplice riadattamento del dato all'input richiesto dal modello. Infine, avviene la conversione in un tensore PyTorch.
        \end{enumerate}
        \item \textbf{Gestione delle Anomalie}: Se una qualsiasi operazione sul file fallisce (file illeggibile o corrotto), viene cattura l'eccezione. Piuttosto che interrompere l'intero training, lo script stampa a video un avviso con il nome del file problematico e restituisce un tensore  di zeri (corrispondente ad un'immagine nera).
    \end{enumerate}
L'adozione di questa architettura rende l'intero processo efficiente in termini di tempo e robusto in caso di errori nel dataset. Il tutto è stato fondamentale per l'intero progetto: i dati scientifici sono stati utilizzati  come se fossero immagini.\\
Il dettaglio implementativo completo della classe è riportato nel Codice \ref{lst:dataset_h5_full} in Appendice \ref{app:codice}

\subsection{Integrazione del Data Loader nella Logica di Training}
La predisposizione del modulo di caricamento dati è un passo necessario, ma non sufficiente per l'adattamento completo del modello. Affinchè la nuova logica di gestione dei dati scientifici divenga operativa, è stato necessario integrarla organicamente nel flusso di lavoro.\\
Di conseguenza, la strategia adottata ha previsto un'estensione strutturale dei motori di addestramento e validazione. L'obiettivo è stato quello di rendere l'architettura capace di accogliere e processare i tensori magnetografici, gestendo le specificità del formato scientifico senza compromettere la stabilità delle procedure di ottimizzazioni originali.

\subsubsection{Adattamento della Logica di Training}
L'estensione del modulo di caricamento dati ha richiesto un adeguamento strutturale del motore di addestramento. L'obiettivo primario è stato quello di rendere il modello capace di selezionare dinamicamente la strategia di ingestione dei dati più appropriata - standard o scientifica - senza alterare il flusso logico del ciclo di addestramento. Per ottenere ciò, sono state apportate le seguenti strategie architetturali:
\begin{itemize}
    \item \textbf{Integrazione Modulare delle Risorse} \\
    Il modulo di caricamento personalizzato è stato integrato nell'ambiente di addestramento in modo condizionale, rendendo le classi disponibili al sistema solo al momento dell'inizializzazione del dataset, garantendo una gestione pulita delle dipendenze.\\
    Vedi Codice \ref{lst:train_import} in Appendice \ref{app:codice}.
    \item \textbf{Strategia di Assemblaggio dei Batch (\textit{Batch Collation})}\\
    Di fondamentale importanza è stata l'implementazione di una logica di aggregazione personalizzata per gestire la corretta formazione dei lotti di dati (\textit{batch}). Quando il sistema raggruppa più campioni, è fondamentale mantenere un'associazione univoca tra tensori magnetici e relative etichette spaziali. La procedura sviluppata assegna un indice posizionale a ciascuna annotazione all'interno del batch, risolvendo le ambiguità strutturali tipiche dei dati complessi. Questo ordinamento permette al modello di correlare le predizioni con le \textit{ground truth} per ogni singolo magnetogramma, passaggio essenziale per il calcolo della funzione di perdita (\textit{loss function}).\\
    La logica di assemblaggio del batch è illustrata nel Codice \ref{lst:train_collate} in Appendice \ref{app:codice}.
    \item \textbf{Attivazione Contestuale tramite Configurazione} \\
    Per garantire una completa flessibilità del sistema, piuttosto che utilizzare un argomento da riga di comando, è stato implementato un meccanismo di \textit{attivazione contestuale} basato sul file di configurazione del dataset. All'avvio, il sistema analizza i metadati di configurazione: la presenza di un identificatore specifico (\textit{flag}) istruisce il framework sulla natura scientifica del dato, innescando automaticamente il meccanismo dedicato senza richiedere interventi manuali.\\
    Quanto descritto è mostrato nel Codice \ref{lst:train_config} in Appendice \ref{app:codice}.
    \item \textbf{Selezione Dinamica del Flusso di Caricamento Dati} \\
    Il cambiamento più significativo riguarda la logica di inizializzazione del flusso dati. Grazie al meccanismo di riconoscimento descritto al punto precedente, è stata inserita una struttura di controllo che opera a runtime:
    \begin{itemize}
        \item in presenza di dati scientifici, il sistema ignora la funzione di caricamento standard ed istanzia un modulo personalizzato, con la strategia di assemblaggio specifica;
        \item in caso contrario, il sistema mantiene inalterato il suo comportamento originale.
    \end{itemize}
    La selezione condizionale del Data Loader è riportata nel Codice \ref{lst:train_loader_choice} in Appendice \ref{app:codice}.
    \item \textbf{Normalizzazione Condizionale}\\
    Una modifica critica ha riguardato il pre-processing interno al ciclo di addestramento. La logica originale di YOLOv7 presuppone che il data loader restituisca immagini in formato 8-bit (valori da 0 a 255) e, pertanto, applica una normalizzazione dividendo il tensore delle immagini per 255.0. Poiché la logica personalizzata esegue già una normalizzazione, tale ulteriore divisione avrebbe reso i dati inadeguati per l'uso. Grazie all'utilizzo del medesimo flag, è stata introdotta una logica condizionale che salta tale operazione di divisione per i magnetogrammi, mantenendola attiva per i dataset standard.
\end{itemize}
La disattivazione della normalizzazione standard è visibile nel Codice \ref{lst:train_norm} in Appendice \ref{app:codice}.\\
L'approccio metodologico adottato segue il paradigma del \textit{feature flagging}: una tecnica di ingegneria del software che permette di attivare funzionalità specifiche di un'applicazione senza dover apportare modifiche significative. Questo ha garantito l'estensione delle capacità di YOLOv7 in modo modulare, mantenendo la totale retrocompatibilità con l'architettura originale e assicurando che la nuova logica sia eseguita solo quando esplicitamente richiesto dal contesto dei dati.

\subsubsection{Estensione della Logica di Validation}
Per mantenere una coerenza all'interno dell'intero ciclo di apprendimento,è stato necessario estendere le funzionalità del motore di validazione. Un modello addestrato su dati scientifici deve essere necessariamente validato utilizzando le medesime procedure di pre-elaborazione e caricamento adottate durante il training. Per questo motivo, sono state implementate modifiche strutturali speculari rispetto a quelle introdotte in fase di addestramento:
\begin{itemize}
    \item \textbf{Integrazione delle Risorse di Validazione} \\
    Analogamente alla fase di training, le classi necessarie per la gestione di dati scientifici sono state rese disponibili all'ambiente di validazione tramite importazioni condizionali, assicurando che il modulo di caricamento specifico sia accessibile solo quando richiesto dalla configurazione.\\
    Vedi Codice \ref{lst:test_import} in Appendice \ref{app:codice}.
    \item \textbf{Assemblaggio Semplificato dei Batch} \\
    È stata definita una logica di aggregazione dei dati ottimizzata per la fase di inferenza. A differenza del training, dove l'associazione indice-etichetta è cruciale per il calcolo della \textit{loss}, la validazione richiede un processo più snello. Pertanto, è stata implementata una procedura semplificata, con lo scopo di raggruppare i tensori caricati in un unico batch standard, massimizzando l'efficienza computazionale durante la valutazione.\\
    L'implementazione semplificata è nel Codice \ref{lst:test_collate} in Appendice \ref{app:codice}.
    \item \textbf{Flag di Controllo Interno}\\
    La funzione principale di test è stata estesa per accettare un nuovo parametro di controllo (\textit{flag}) booleano. Quest'ultimo è trasmesso dal motore di addestramento al termine di ogni epoca per segnalare la natura dei dati in arrivo. Tale indicatore non attiva il caricamento dei dati scientifici, bensì governa i comportamenti successivi del sistema (ad esempio la normalizzazione delle immagini e la visualizzazione dei risultati), assicurando che i dati scientifici vengano trattati con le procedure appropriate.\\
    La modifica alla firma della funzione è mostrata nel Codice \ref{lst:test_param} in Appendice \ref{app:codice}.
    \item \textbf{Creazione Dinamica del Data Loader} \\
    Per conferire flessibilità allo script di validazione, è stato implementato un blocco di istanziazione dinamica. Il sistema legge il file di configurazione del dataset e, in base alla presenza della chiave specifica per i dati scientifici:
    \begin{itemize}
        \item in caso positivo, istanzia il modulo personalizzato con la strategia di assemblaggio semplificata;
        \item in caso negativo (o in assenza della chiave), esegue la funzione di caricamento originale, mantenendo la totale compatibilità con i dataset di immagini tradizionali.
    \end{itemize}
    La logica di creazione del loader è nel Codice \ref{lst:test_loader_choice} in Appendice \ref{app:codice}.
\item \textbf{Normalizzazione Condizionale} \\
Per evitare alterazioni numeriche dei dati, la normalizzazione dei dei pixel è applicata in modo selettivo. Mentre le immagini standard (0-255) richiedono una divisione per essere portate nel range unitario, i magnetogrammi arrivano al modulo di validazione già normalizzati. Grazie al \texttt{flag di controllo}, il sistema inibisce la divisione standard in presenza di dati scientifici, evitando una doppia normalizzazione che comprometterebbe la corretta scala dei valori.\\
Il bypass della divisione per 255 è nel Codice \ref{lst:test_norm} in Appendice \ref{app:codice}.
\item \textbf{Riscalamento delle Coordinate}\\
Un intervento critico ha riguardato la logica di proiezione delle bounding box (sia quelle predette dal modello che quelle reali). Il modulo personalizzato restituisce le dimensioni originali in un formato diretto (lista \texttt{[H,W]}), differente dalla struttura nidificata utilizzata dal data loader standard. Per allineare il codice a questo formato, le chiamate alla funzione di riscalamento sono state semplificate, rimuovendo gli indici non più necessari. Questo garantisce che il confronto tra le predizioni del modello ed etichette di verità avvenga correttamente nello spazio pixel originale dell'immagine, assicurando la validità delle metriche di valutazione.\\
L'adattamento delle coordinate è visibile nel Codice \ref{lst:test_coords} in Appendice \ref{app:codice}.
\item \textbf{Visualizzazione Scientifica} \\
Infine, per garantire che i risultati della validazione siano visivamente corretti ed interpretabili, le chiamate alle funzioni di plotting sono state modificate per supportare il rendering scientifico dei magnetogrammi (mappatura dei colori e correzione geometrica).\\
La chiamata modificata alla funzione di plotting è nel Codice \ref{lst:test_plot_call} in Appendice \ref{app:codice}.
\end{itemize}

\subsubsection{Personalizzazione delle Utility di Visualizzazione}
L'ultimo tassello nell'adattamento del framework ha riguardato la corretta rappresentazione grafica dei risultati all'interno del modulo di visualizzazione. Nella sua versione originale, il sistema è progettato per operare esclusivamente su immagini a tre canali (BGR), presupponendo che l'input sia sempre un'immagine standard.\\
Applicare questa logica direttamente ai magnetogrammi - che sono dati scientifici a singolo canale rappresentanti l'intensità del campo magnetico - avrebbe prodotto un output visivamente incomprensibile (falsi colori e perdita di contrasto), rendendo impossibile la distinzione delle polarità, informazione cruciale per l'analisi fisica. \\
L'obiettivo è stato, quindi, quello di integrare una logica di visualizzazione personalizzata che si attiva esclusivamente in presenza di dati scientifici. Tale logica include l'applicazione di mappe cromatiche specifiche, la correzione dell'orientamento geometrico e il conseguente adattamento delle coordinate delle bounding box. Allo stesso tempo, è stata mantenuta la piena compatibilità con i dataset standard.\\
Per raggiungere questo scopo, la procedura di visualizzazione è stata modificata attraverso i seguenti interventi:
\begin{itemize}
    \item \textbf{Estensione della Firma della Funzione}\\
    La prima modifica ha riguardato l'interfaccia della procedura di plotting. È stato introdotto un parametro opzionale di controllo, il quale agisce come una \textit{feature flag} o \texttt{indicatore di contesto}: esso permette ai motori di addestramento e validazione di segnalare al visualizzatore che il batch in arrivo contiene dati scientifici (magnetogrammi) e non immagini standard. Il valore di default impostato a falso garantisce che la funzione mantenga inalterato il suo comportamento in assenza di specifiche istruzioni.\\
    Vedi Codice \ref{lst:plot_sig} in Appendice \ref{app:codice}.
    \item \textbf{Normalizzazione Condizionale dell'Input}\\
    Il flusso originale prevedeva una de-normalizzazione sistematica dei tensori di input (moltiplicazione per 255), assumendo che questi fossero normalizzati nel range \texttt{[0,1]} per la visualizzazione a 8-bit. Sebbene corretta per le immagini, questa operazione altera erroneamente i magnetogrammi. La modifica incapsula questa operazione in un blocco condizionale: la de-normalizzazione viene eseguita solo se il dato non è indicato come scientifico, preservando l'integrità numerica dei magnetogrammi per il successivo rendering.\\
    Il controllo condizionale è nel Codice \ref{lst:plot_norm} in Appendice \ref{app:codice}.
    \item \textbf{Logica di Rendering Personalizzata per Magnetogrammi}\\
    Si tratta dell'intervento più sostanziale. All'interno del ciclo che processa ogni immagine del batch, è stata inserita una diramazione logica. Se il \texttt{flag scientifico} è attivo, la logica di visualizzazione standard viene sostituita da un \textit{processo di rendering} specifico per i dati scientifici:
    \begin{enumerate}
        \item \textbf{Ripristino della Scala Fisica}: i dati sono riportati alla loro scala fisica originale (da [0,1] a [-1500,1500]), utilizzando i parametri di clipping definiti nel Data Loader.
        \item \textbf{Mappatura Cromatica}: viene applicata una mappa di colori divergente (denominata \texttt{seismic}), specifica per evidenziare le polarità magnetiche con colori contrastanti (rosso per il positivo e blu per il negativo).
        \item \textbf{Conversione RGB}: l'immagine colorata (che ha 4 canali RGBA) è convertita in un'immagine RGB a 8 bit, compatibile con le librerie grafiche.
        \item \textbf{Correzione dell'Orientamento}: l'immagine viene capovolta verticalmente (\textit{flip}) per allinearsi alle convenzioni di visualizzazione astrofisica standard, dove l'asse Y cresce verso l'alto (contrariamente alle immagini digitali).
    \end{enumerate}
    Il blocco alternativo mantiene la logica originale per le immagini standard.\\
    L'algoritmo di rendering e colorazione è dettagliato nel Codice \ref{lst:plot_render} in Appendice \ref{app:codice}.
    \item \textbf{Adattamento delle Coordinate delle Bounding Box}\\
    Come conseguenza diretta dell'inversione verticale dell'immagine del magnetogramma (descritta al punto precedente), anche le ordinate spaziali delle bounding box necessitano di una trasformazione speculare. Prima che le etichette vengano disegnate sull'immagine, è stato aggiunto un blocco di calcolo che esegue un'inversione matematica. Questo assicura che le etichette (sia le predizioni del modello che la \textit{ground truth}) rimangano geometricamente coerenti con la nuova rappresentazione visiva del magnetogramma.\\
    La correzione delle coordinate delle bounding box è nel Codice \ref{lst:plot_flip} in Appendice \ref{app:codice}.
\end{itemize}

\section{Approccio alternativo: Pre-conversione del Dataset}
In alternativa all'adattamento architetturale di YOLOv7 (descritto ampiamente nella sezione~\ref{sec:adattamento_yolo}), è stato esplorato un approccio metodologico che inverte il paradigma di adattamento: \textit{conformare i dati al framework, piuttosto che il framework ai dati}.\\
Sostanzialmente, si è scelto di trattare il modello preesistente come una \textit{black box}, lasciandone il codice sorgente inalterato. Per conseguire tale obiettivo, è stato implementato un processo di pre-conversione strutturato in due fasi, volto a trasformare l'intero dataset dal formato gerarchico HDF5 al \textbf{formato YOLO}. Quest'ultimo impone una struttura specifica:
\begin{itemize}
    \item Per ogni magnetogramma (in formato immagine .jpg), deve esistere un file di testo corrispondente con il medesimo identificativo.
    \item Ogni riga del file testuale rappresenta una singola bounding box presente nella relativa immagine.
    \item Ogni riga è espressa nel formato \\
    \texttt{<class\_id> <x\_centro> <y\_centro> <larghezza> <altezza>}.
\end{itemize}
Per lo scopo, è stato sviluppato un flusso di lavoro specifico.

\subsection{Prima Fase: Transcodifica da HDF5 a YOLO}
La prima fase è governata da una \textbf{routine di elaborazione batch}, progettata per scansionare l'intero archivio dati ed eseguire, su ogni campione scientifico, due flussi di trasformazione paralleli.

\subsubsection{Estrazione e Formattazione delle Annotazioni}
Analogamente a quanto implementato nella logica di caricamento dati (descritta nella sezione~\ref{sec:dataset_h5}), il sistema naviga la struttura  gerarchica del file per recuperare i metadati spaziali di ogni regione attiva. Vengono eseguiti i medesimi controlli di validità (verifica della completezza degli attributi e consistenza dimensionale) e il calcolo delle coordinate normalizzate. Le coordinate risultanti vengono infine serializzate come stringhe di testo e salvate nel file testuale di annotazione corrispondente.

\subsubsection{Trasformazione e Quantizzazione del Segnale}
Contestualmente, il modulo processa la matrice dei dati grezzi contenuti nel file scientifico. Questo processo converte l'informazione fisica in un'immagine standard, seguendo una serie di passi:
\begin{enumerate}
    \item \textbf{Pulizia Dati}: rimozione di valori non validi (\texttt{Nan} e \texttt{inf});
    \item \textbf{Clipping Fisico}: saturazione dei valori di campo magnetico nell'intervallo di interesse (\texttt{[-1500,1500]});
    \item \textbf{Normalizzazione Lineare}: riscalamento dei dati nell'intervallo unitario (\texttt{[0.0,1.0]});
    \item \textbf{Ridimensionamento Spaziale}: interpolazione dell'immagine alla risoluzione target (formato 640x640 pixel).
\end{enumerate}
Il \textit{passaggio chiave} avviene al termine di questa catena: i dati in virgola mobile (\texttt{float}) vengono convertiti in interi a 8-bit. Questa operazione di \textit{quantizzazione} riduce la vasta gamma dinamica dei dati scientifici in 256 valori discreti, rendendoli compatibili con i formati immagine standard. Il risultato viene duplicato su tre canali e codificato con compressione JPEG.\\
La procedura viene reiterata per le tre partizioni del dataset, producendo in output tre archivi distinti, ciascuno organizzato secondo la gerarchia standard di cartelle richiesta dal framework.\\
Il codice completo della procedura di conversione è riportato nel Codice \ref{lst:script_preprocess} in Appendice \ref{app:codice}.

\subsection{Seconda Fase: Post-Processing e Mascheramento dello Sfondo}
La procedura di conversione appena descritta introduce un \textit{artefatto sistematico} legato alla gestione dei valori nulli dello sfondo. Durante la fase di pre-processing, i valori esterni al disco solare (\texttt{NaN}) vengono sostituiti con il valore neutro \texttt{0.0}. A seguito della normalizzazione Min-Max nell'intervallo \texttt{[-1500,1500]}, tale valore zero si colloca al centro del range dinamico \texttt{0.5} che, nella conversione finale a 8-bit, viene mappato nel valore 127.\\
Il risultato è un'immagine in cui lo sfondo, anziché essere nero, appare come un \textbf{grigio medio}, risultando indistinguibile dalle aree inattive del disco solare stesso (anch'esso rappresentato in scala di grigi). Questa ambiguità visiva riduce drasticamente il contrasto e introduce rumore di fondo che compromette la convergenza dell'addestramento, come evidenziato in figura~\ref{fig:artefatto_grigio}.
\begin{figure}[H]
  \centering
  \includegraphics[width=0.5\textwidth]{Figs/Cap3/esempio_con_artefatto.jpg}
  \caption[Artefatto di conversione H5] 
  {Esempio visivo dell'artefatto generato dalla routine di conversione primaria. Come si può osservare, lo sfondo è renderizzato come un grigio medio, rendendo il confine del disco solare (anch'esso in scala di grigi) visivamente ambiguo. Questa mancanza di bordo netto introduce rumore spuro nel dataset.}
  \label{fig:artefatto_grigio}
\end{figure}
Per risolvere questa criticità, è stato implementato un \textbf{modulo di filtraggio ottico} (post-processing), il cui scopo è isolare geometricamente il disco solare, forzando lo sfondo esterno ad un valore di nero assoluto (0).\\
La procedura esegue le seguenti operazioni su ciascun campione generato:
\begin{enumerate}
    \item \textbf{Definizione Geometrica del Raggio}: il sistema identifica il centro dell'immagine e calcola il raggio ottimale basandosi sulla dimensione minore del fotogramma. Tale raggio viene ridotto di un \textit{fattore percentuale} (impostato al 96\%) per escludere i bordi rumorosi.
    \item \textbf{Generazione della Maschera}: viene inizializzata una matrice binaria (\textit{maschera}) completamente nera, avente le medesime dimensioni dell'immagine target.
    \item \textbf{Delimitazione della ROI}: sulla maschera viene disegnato un cerchio bianco pieno (valore 1), corrispondente alla Regione di Interesse (ROI) che contiene il disco solare da preservare.
    \item \textbf{Applicazione del Filtro}: viene eseguita un'operazione logica bit-a-bit (\textit{bitwise AND}) tra l'immagine originale e la maschera. Questa operazione preserva i pixel all'interno della ROI e azzera forzatamente tutti i pixel esterni, ripristinando il nero puro.
\end{enumerate}
L'algoritmo per l'applicazione della maschera circolare è consultabile nel Codice \ref{lst:script_convert} in Appendice \ref{app:codice}.\\
L'efficacia di questo intervento è immediata: il mascheramento ripristina il corretto rapporto segnale-rumore, garantendo che la rete neurale processi esclusivamente le informazioni pertinenti, come mostrato in figura~\ref{fig:immagine_mascherata}
\begin{figure}[H]
  \centering
  \includegraphics[width=0.5\textwidth]{Figs/Cap3/esempio_mascherato.jpg}
  \caption[Risultato della maschera circolare] 
  {Risultato dell'applicazione del modulo di mascheramento sull'immagine precedentemente affetta da artefatti (mostrata nella figura~\ref{fig:artefatto_grigio}). L'operazione ha rimosso con successo lo sfondo grigio, forzandolo a nero puro. L'immagine ora presenta un contrasto netto tra il disco solare e lo sfondo, fornendo un input di addestramento pulito e privo di ambiguità.}
  \label{fig:immagine_mascherata}
\end{figure}

\section{Tracking Multi-Oggetto con Norfair}
Solo dopo aver terminato l'adattamento di YOLOv7, è stato possibile sviluppare un'applicazione in grado di utilizzare il suddetto modello addestrato per eseguire un'analisi temporale. Le regioni attive solari, infatti, non sono entità statiche, ma evolvono nel tempo, spostandosi e modificando la loro morfologia attraverso la rotazione solare.\\
Per tracciare tali evoluzioni, è stata integrata -come ampliamente anticipato- la libreria \textbf{Norfair}, una soluzione modulare che implementa un approccio di tipo \textit{tracking-by-detection}. In questo paradigma, il modello neurale (YOLOv7) opera su ogni singolo fotogramma in modo indipendente, mentre il modulo di tracciamento si occupa di associare le rilevazioni tra istanti temporali consecutivi, assegnando un identificativo univoco (ID) a ciascun oggetto e mantenendolo consistente nel tempo.

\subsection{Implementazione del Modulo di Tracciamento}
Il cuore del sistema è costituito da un processo di elaborazione sequenziale progettato per processare serie temporali di magnetogrammi. Il funzionamento si articola in quattro fasi logiche:
\begin{enumerate}
    \item \textbf{Gestione Ibrida dei Dati}\\
    Il modulo è stato ingegnerizzato per gestire indifferentemente sia dati scientifici (HDF5) che immagini standard. Nel primo caso, viene applicata in tempo reale la medesima procedura di pre-processing utilizzata durante il training (gestione dei valori nulli, clipping fisico e normalizzazione), garandendo la totale congruenza statistica tra i dati appresi dal modello e quelli in ingresso al tracker.
    \item \textbf{Inferenza e Transcodifica}\\
    Per ogni frame, la rete neurale genera le bounding box delle regioni attive. Una routine di interfaccia converte i tensori di output originali in oggetti strutturati compatibili con il motore di tracciamento, preservando le coordinate spaziali ed i punteggi di confidenza.
    \item \textbf{Associazione Temporale}\\
    Il tracker calcola la distanza tra le rilevazioni correnti e la stima della posizione degli oggetti tracciati nei frame precedenti. Data la natura fisica del problema (le macchie solari si muovono con dinamiche fluide e prevedibili), è stata adottata la metrica \textit{IoU} (Inserction over Union) come criterio di associazione: se la sovrapposizione spaziale tra la predizione attuale e quella passata supera una soglia critica, l'identità dell'oggetto viene preservata.
    \item \textbf{Sintesi dell'Output}\\
    Al termine del processo, il sistema aggrega i risultati visuali, generando un video riassuntivo che mostra l'evoluzione dinamica delle regioni attive, con i relativi colori identificativi sovraimpressi per facilitare l'analisi visiva.
\end{enumerate}
L'implementazione algoritmica del tracciamento è riportata nel Codice \ref{lst:track_script} in Appendice \ref{app:codice}.

\subsection{Valutazione Quantitativa delle Prestazioni}
Oltre all'analisi qualitativa visiva, è stato necessario quantificare la robustezza del sistema di tracciamento. A tal fine, è stato sviluppato un modulo di validazione dedicato al calcolo delle metriche standard per il Multi-Object Tracking (MOT), tra cui la \textit{MOTA} (Accuracy) e l'\textit{IDF1} (F1 Score sull'identificazione).\\
Poiché il calcolo di tali metriche richiede uno standard di annotazione specifico, il modulo implementa una routine di \textbf{standardizzazione automatica} che esegue tre compiti critici:
\begin{itemize}
    \item \textbf{Conversione della Ground Truth}: Legge le annotazioni proprietarie e le trasforma nel formato standard internazionale (MOTChallenge), allineando coordinate e identificativi temporali.
    \item \textbf{Inizializzazione dell'Ambiente di Test}: Configura gli accumulatori statici necessari per registrare le associazioni corrette, le mancate rilevazioni e gli scambi di identità (\textit{ID switches}).
    \item \textbf{Benchmarking Frame-by-Frame}: Confronta sequenzialmente le traiettorie generate dal tracker con i dati reali, producendo un report analitico dettagliato.
\end{itemize}
Questa infrastruttura di test permette do valutare oggettivamente la capacità del modello di mantenere stabile l'identità di una regione attiva anche in presenza di variazioni morfologiche rapide o occlusioni spaziali. \\
Il codice relativo al calcolo delle metriche è consultabile nel Codice \ref{lst:track_metrics} in Appendice \ref{app:codice}.

\section{Introduzione alla Sperimentazione}
In questo capitolo, sono state presentate le strategie per adattare YOLOv7 al dominio dell'astrofisica solare, esplorando un \textit{duplice approccio}: da un lato, l'estensione profonda del framework per la gestione nativa dei dati scientifici, utilizzata come \textit{proof-of-concept} per validare la fattibilità tecnica dell'apprendimento su dati grezzi; dall'altro, lo sviluppo di un processo di pre-elaborazione per la normalizzazione e quantizzazione dei magnetogrammi. Inoltre, sono stati illustrati i moduli integrati per la visualizzazione dei risultati fisici e per il tracciamento temporale delle regioni attive.\\
Nel prossimo capitolo, verranno analizzati i risultati della sperimentazione, basati sulla \textbf{strategia di pre-conversione del dataset}: tale approccio, infatti, si è rivelato il più idoneo per una sperimentazione estensiva, permettendo di valutare la robustezza del sistema tramite le metriche di rilevamento e tracciamento su larga scala.